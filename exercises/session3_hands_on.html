<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Session 3: Testing & Production - Hands-on Exercises</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #9b59b6;
            padding-bottom: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 30px;
            border-left: 4px solid #9b59b6;
            padding-left: 10px;
        }
        
        h3 {
            color: #7f8c8d;
            margin-top: 20px;
        }
        
        .info-box {
            background-color: #e8f4fd;
            border-left: 4px solid #3498db;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }
        
        .warning-box {
            background-color: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }
        
        .challenge-box {
            background-color: #fff;
            border: 2px solid #9b59b6;
            padding: 20px;
            margin: 30px 0;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        .optional-challenge {
            background-color: #ffeaa7;
            border: 2px dashed #fdcb6e;
            padding: 15px;
            margin: 15px 0;
            border-radius: 8px;
        }
        
        .feast-box {
            background-color: #f4e7ff;
            border: 2px solid #9b59b6;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }
        
        code {
            background-color: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
        }
        
        pre {
            background-color: #2c3e50;
            color: #ecf0f1;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            margin: 15px 0;
        }
        
        .solution {
            display: none;
            background-color: #e8f8f5;
            border: 2px solid #27ae60;
            padding: 15px;
            margin-top: 10px;
            border-radius: 5px;
        }
        
        .solution-button {
            background-color: #9b59b6;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 5px;
            cursor: pointer;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .solution-button:hover {
            background-color: #8e44ad;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
            background-color: white;
        }
        
        th, td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        
        th {
            background-color: #9b59b6;
            color: white;
        }
        
        tr:nth-child(even) {
            background-color: #f2f2f2;
        }
        
        .highlight {
            background-color: #ffeb3b;
            padding: 2px 4px;
            border-radius: 3px;
        }
        
        .recap-box {
            background-color: #f0f3f4;
            border: 2px solid #34495e;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }
        
        .instructor-note {
            background-color: #ffe5e5;
            border: 2px dashed #ff6b6b;
            padding: 15px;
            margin: 15px 0;
            border-radius: 8px;
        }
    </style>
    <script>
        function toggleSolution(id) {
            var solution = document.getElementById(id);
            var button = event.target;
            if (solution.style.display === 'none' || solution.style.display === '') {
                solution.style.display = 'block';
                button.textContent = 'Hide Solution';
            } else {
                solution.style.display = 'none';
                button.textContent = 'Show Solution';
            }
        }
    </script>
</head>
<body>
    <h1>üöÄ Session 3: Testing & Production - Hands-on Exercises</h1>
    
    <div class="info-box">
        <h3>‚è±Ô∏è Duration: 45-60 minutes</h3>
        <p><strong>Prerequisites:</strong> Completed Session 1 & 2 models</p>
        <p><strong>Goal:</strong> Add production-grade testing, monitoring, and enrichment</p>
        <p><strong>Note:</strong> Shorter session to leave time for Feast production discussion</p>
    </div>

    <div class="recap-box">
        <h2>üìã Quick Recap from Sessions 1-2</h2>
        <p><strong>What We've Built:</strong></p>
        <ul>
            <li>‚úÖ Staging tables with proper types</li>
            <li>‚úÖ Customer landing and daily features</li>
            <li>‚úÖ Incremental models with late data handling</li>
            <li>‚úÖ Snapshots for slowly changing dimensions</li>
        </ul>
        <p><strong>Today's Focus:</strong> Make it bulletproof with comprehensive testing and production features!</p>
    </div>

    <div class="info-box">
        <h3>üí° Need to Catch Up or Reset?</h3>
        <p>If you're joining this session or want to start fresh with all Session 2 models in place (including snapshots!), you can use the reset script:</p>
        <pre>./reset_to_session.sh 2</pre>
        <p>This will reset your project to the end of Session 2 with all foundation models, variables, snapshots, and advanced patterns ready to go.</p>
        <p><strong>‚ö†Ô∏è Use Wisely:</strong> The reset script will overwrite any custom work or bonus challenges you've completed. Make sure to save any extra code you want to keep before running it!</p>
    </div>

    <!-- Challenge 1: Seeds for Reference Data -->
    <div class="challenge-box">
        <h2>üéØ Challenge 1: Seeds for Business Logic</h2>
        <p>Use seed files to manage reference data that enriches your ML features with geographic information.</p>

        <div class="info-box">
            <h3>üìã Concrete Instructions</h3>
            <p><strong>What you'll create:</strong></p>
            <ul>
                <li><strong>Geographic data:</strong> Brazilian city tiers and economic zones</li>
                <li><strong>Feature enrichment:</strong> Join this data to your customer features</li>
                <li><strong>Testing:</strong> Validate the enrichment works correctly</li>
            </ul>

            <p><strong>Why this matters for ML:</strong> Business rules as code, version controlled, testable</p>
        </div>

        <h3>Step 1: Create the seed file</h3>
        <p>First, you'll need a seed file with Brazilian city geographic data. Create <code>seeds/brazil_cities.csv</code> with the following content:</p>

        <pre>
city,state,region,population_tier,economic_zone,logistics_hub
S√£o Paulo,SP,Southeast,mega,primary,1
Rio de Janeiro,RJ,Southeast,mega,primary,1
Bras√≠lia,DF,Central-West,large,primary,1
Salvador,BA,Northeast,large,secondary,1
Fortaleza,CE,Northeast,large,secondary,1
Belo Horizonte,MG,Southeast,large,secondary,1
Manaus,AM,North,large,tertiary,1
Curitiba,PR,South,large,secondary,1
Recife,PE,Northeast,large,secondary,1
Porto Alegre,RS,South,large,secondary,1
Bel√©m,PA,North,medium,tertiary,0
Goi√¢nia,GO,Central-West,medium,tertiary,0
Guarulhos,SP,Southeast,medium,primary,1
Campinas,SP,Southeast,medium,primary,1
S√£o Lu√≠s,MA,Northeast,medium,tertiary,0</pre>

        <h3>Requirements:</h3>
        <ol>
            <li>Use the seed file data above to create <code>seeds/brazil_cities.csv</code></li>
            <li>Load seeds into your database with <code>dbt seed</code></li>
            <li>Create an enriched mart table (<code>models/mart/mart_customer_features_enriched.sql</code>) that joins your customer features with the seed file. The mart should get the latest features for each customer and add geographic attributes like city tier, economic zone, and logistics hub status.</li>
            <li>Test that enrichment is working correctly</li>
        </ol>

        <button class="solution-button" onclick="toggleSolution('solution1')">Show Solution</button>
        <div id="solution1" class="solution">
            <h4>Step 1: Load the seeds</h4>
            <pre>
# Load the seed file into your database
dbt seed
</pre>

            <h4>Step 2: Create enriched mart table</h4>
            <pre>
{{ config(
    materialized='table',
    post_hook=[
        "CREATE INDEX IF NOT EXISTS idx_{{ this.name }}_city_tier
         ON {{ this }} (city_tier, economic_zone)",
        "ANALYZE {{ this }}"
    ]
) }}

WITH latest_features_ranked AS (
    -- Get most recent features for each customer with ranking
    SELECT
        customer_id,
        date,
        total_payment_value as customer_revenue,
        orders_7d,
        orders_14d as total_orders,
        days_since_landing,
        landing_date,
        ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY date DESC) as rn
    FROM {{ ref('int_customer_daily_features') }}
),

latest_features AS (
    -- Filter to only the most recent record per customer
    SELECT
        customer_id,
        date,
        customer_revenue,
        orders_7d,
        total_orders,
        days_since_landing,
        landing_date
    FROM latest_features_ranked
    WHERE rn = 1
),

customer_locations AS (
    -- Get customer city/state information
    SELECT
        customer_id,
        customer_city as city,
        customer_state as state
    FROM {{ ref('stg_customers') }}
)

SELECT
    f.*,
    l.city,
    l.state,
    -- Geographic enrichment from brazil_cities seed
    COALESCE(c.region, 'Unknown') as region,
    COALESCE(c.population_tier, 'unknown') as city_tier,
    COALESCE(c.economic_zone, 'unknown') as economic_zone,
    COALESCE(c.logistics_hub, 0) as is_logistics_hub,

    -- Add derived features based on enrichment
    CASE
        WHEN c.economic_zone = 'primary' THEN 'high_opportunity'
        WHEN c.economic_zone = 'secondary' THEN 'medium_opportunity'
        ELSE 'emerging_market'
    END as market_opportunity,

    CASE
        WHEN COALESCE(c.logistics_hub, 0) = 1 AND f.customer_revenue > 1000 THEN 'premium_fast_delivery'
        WHEN COALESCE(c.logistics_hub, 0) = 1 THEN 'standard_fast_delivery'
        WHEN f.customer_revenue > 1000 THEN 'premium_standard'
        ELSE 'standard'
    END as service_tier,

    -- Business score combining revenue and geography
    CASE
        WHEN f.customer_revenue > 2000 AND c.economic_zone = 'primary' THEN 100
        WHEN f.customer_revenue > 2000 THEN 85
        WHEN f.customer_revenue > 1000 AND c.economic_zone = 'primary' THEN 75
        WHEN f.customer_revenue > 1000 THEN 65
        WHEN f.customer_revenue > 500 THEN 50
        WHEN f.customer_revenue > 200 THEN 35
        ELSE 20
    END as business_priority_score

FROM latest_features f
LEFT JOIN customer_locations l
    ON f.customer_id = l.customer_id
LEFT JOIN {{ ref('brazil_cities') }} c
    ON LOWER(TRIM(l.city)) = LOWER(TRIM(c.city))</pre>

            <h4>Step 3: Run and test the enrichment</h4>
            <p>Now run the mart model to materialize the enriched table, then verify that the seed data is being correctly joined to customer features.</p>
            <pre>
# Run the enriched mart
dbt run --select mart_customer_features_enriched

# Verify seeds are being used correctly
SELECT
    city_tier,
    economic_zone,
    market_opportunity,
    COUNT(*) as customer_count,
    AVG(customer_revenue) as avg_revenue,
    AVG(business_priority_score) as avg_priority_score
FROM mart_customer_features_enriched
GROUP BY city_tier, economic_zone, market_opportunity
ORDER BY avg_priority_score DESC;

# Check for customers without enrichment (troubleshooting)
SELECT
    COUNT(*) as total_customers,
    COUNT(CASE WHEN city_tier = 'unknown' THEN 1 END) as unknown_city_count,
    ROUND(100.0 * COUNT(CASE WHEN city_tier = 'unknown' THEN 1 END) / COUNT(*), 2) as unknown_city_percent,
    COUNT(CASE WHEN is_logistics_hub = 1 THEN 1 END) as logistics_hub_customers
FROM mart_customer_features_enriched;</pre>
        </div>

        <div class="optional-challenge">
            <h3>‚≠ê Optional Challenge: Seasonal Geographic Adjustments</h3>
            <p>Create a seed file with seasonal adjustments to geographic zones for holiday shopping patterns:</p>
            <ul>
                <li>High season priorities (November-December)</li>
                <li>Regional festival adjustments (by state)</li>
                <li>Dynamic logistics capacity</li>
            </ul>
        </div>
    </div>

    <h2>üé≠ Macros in dbt</h2>

    <div class="info-box">
        <h3>Understanding Macros - You've Already Used Them!</h3>

        <p><strong>What are Macros?</strong></p>
        <p>Macros are reusable pieces of SQL code, like functions in programming languages.</p>

        <p><strong>You've Already Used These Macros from Session 1:</strong></p>
        <ul>
            <li><strong>dbt_utils.generate_surrogate_key():</strong> Created unique keys for customer landing</li>
            <li><strong>dbt_utils.date_spine():</strong> Generated date ranges for daily features</li>
            <li><strong>dbt_utils.pivot():</strong> Converted payment types to columns</li>
            <li><strong>Built-in {{ ref() }} and {{ source() }}:</strong> These are macros too!</li>
        </ul>

        <p><strong>Now you'll create your own:</strong></p>
        <pre>
-- macros/get_payment_metrics.sql
{% macro get_payment_metrics(days) %}
    SUM(CASE
        WHEN order_date >= date - {{ days }}
        THEN payment_value
    END) as payment_{{ days }}d
{% endmacro %}</pre>

        <p><strong>Using your macro:</strong></p>
        <pre>
SELECT
    customer_id,
    {{ get_payment_metrics(7) }},
    {{ get_payment_metrics(30) }}
FROM payments</pre>

        <p><strong>Macros vs Functions vs Variables:</strong></p>
        <ul>
            <li><strong>Variables:</strong> Store values (numbers, strings, lists)</li>
            <li><strong>Macros:</strong> Generate SQL code dynamically</li>
            <li><strong>Functions:</strong> SQL functions run in the database</li>
        </ul>
    </div>

    <!-- Challenge 2: Macros - Tier Classification -->
    <div class="challenge-box">
        <h2>üéØ Challenge 2: Create Reusable Tier Classification Macro</h2>
        <p>Build a macro to standardize tier classification logic across different entities - turning the seller tier logic from Session 2 into reusable code.</p>

        <div class="info-box">
            <h3>üéØ The DRY Principle for Business Logic</h3>
            <p><strong>The Challenge:</strong> In Session 2, you manually created tier classifications for sellers using a CASE statement. What if you need the same logic for customers? Products? Regions?</p>

            <p><strong>The Solution:</strong> Macros! Encapsulate business logic once, reuse everywhere.</p>

            <h4>Benefits of Tier Macros:</h4>
            <ul>
                <li><strong>Consistency:</strong> Same tier definitions across all entities</li>
                <li><strong>Maintainability:</strong> Update thresholds in one place</li>
                <li><strong>Reusability:</strong> Apply to sellers, customers, products, etc.</li>
                <li><strong>Business Alignment:</strong> Tier logic matches across reports</li>
            </ul>
        </div>

        <h3>Requirements:</h3>
        <ol>
            <li>Create a <code>macros</code> folder in your project root (if it doesn't exist already)</li>
            <li>Create a macro file <code>macros/classify_tier.sql</code> with a <code>classify_tier()</code> macro</li>
            <li>The macro should accept a metric column and tier type ('volume' or 'revenue')</li>
            <li>Refactor <code>models/intermediate/int_seller_performance.sql</code> from Session 2 to use your new macro</li>
            <li>Create a NEW model <code>models/intermediate/int_customer_tiers.sql</code> that classifies customers using the same macro</li>
        </ol>

        <div class="info-box">
            <h3>üìÅ Project Structure</h3>
            <p>Your dbt project should look like this:</p>
            <pre>
dbt-workshop/
‚îú‚îÄ‚îÄ macros/                    ‚Üê Create this folder
‚îÇ   ‚îî‚îÄ‚îÄ classify_tier.sql     ‚Üê Your macro goes here
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ intermediate/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ int_seller_performance.sql    ‚Üê Refactor this
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ int_customer_tiers.sql        ‚Üê Create this new model
‚îÇ   ‚îî‚îÄ‚îÄ staging/
‚îú‚îÄ‚îÄ seeds/
‚îî‚îÄ‚îÄ dbt_project.yml</pre>
            <p><strong>Note:</strong> Macros are stored separately from models in the <code>macros/</code> folder at the project root.</p>

            <p><strong>To create the macros folder:</strong></p>
            <pre>
# From your dbt project root (dbt-workshop/)
mkdir -p macros</pre>
        </div>

        <div class="info-box">
            <h3>üìä Tier Thresholds</h3>

            <p><strong>Volume-based tiers (order count):</strong></p>
            <ul>
                <li>Platinum: 500+ orders</li>
                <li>Gold: 100-499 orders</li>
                <li>Silver: 20-99 orders</li>
                <li>Bronze: 1-19 orders</li>
            </ul>

            <p><strong>Revenue-based tiers (total spending):</strong></p>
            <ul>
                <li>Platinum: $10,000+</li>
                <li>Gold: $5,000-$9,999</li>
                <li>Silver: $1,000-$4,999</li>
                <li>Bronze: Under $1,000</li>
            </ul>
        </div>

        <button class="solution-button" onclick="toggleSolution('solution2')">Show Solution</button>
        <div id="solution2" class="solution">
            <h4>Step 1: Create the macros folder and macro file</h4>
            <p><strong>Create this file:</strong> <code>macros/classify_tier.sql</code></p>
            <pre>
{% macro classify_tier(metric_column, tier_type='volume') %}
    CASE
        {% if tier_type == 'volume' %}
            -- Volume-based tiers (order count)
            WHEN {{ metric_column }} >= 500 THEN 'platinum'
            WHEN {{ metric_column }} >= 100 THEN 'gold'
            WHEN {{ metric_column }} >= 20 THEN 'silver'
            ELSE 'bronze'
        {% elif tier_type == 'revenue' %}
            -- Revenue-based tiers (total value)
            WHEN {{ metric_column }} >= 10000 THEN 'platinum'
            WHEN {{ metric_column }} >= 5000 THEN 'gold'
            WHEN {{ metric_column }} >= 1000 THEN 'silver'
            ELSE 'bronze'
        {% else %}
            -- Default to volume if unknown type
            WHEN {{ metric_column }} >= 500 THEN 'platinum'
            WHEN {{ metric_column }} >= 100 THEN 'gold'
            WHEN {{ metric_column }} >= 20 THEN 'silver'
            ELSE 'bronze'
        {% endif %}
    END
{% endmacro %}</pre>

            <h4>Step 2: Refactor the seller performance model</h4>
            <p><strong>Edit this file:</strong> <code>models/intermediate/int_seller_performance.sql</code></p>
            <p>Replace the hardcoded CASE statement with the macro:</p>
            <pre>
{{ config(materialized='table') }}

-- Use variable to control analysis date for testing
{% set snapshot_date = var('snapshot_date', '2018-10-17') %}

WITH seller_metrics AS (
    SELECT
        seller_id,
        COUNT(DISTINCT order_id) as total_orders,
        COUNT(DISTINCT product_id) as unique_products,
        SUM(price) as total_revenue,
        AVG(price) as avg_order_value,
        MIN(order_date) as first_sale_date,
        MAX(order_date) as last_sale_date,
        CURRENT_TIMESTAMP as updated_at
    FROM {{ ref('stg_order_items_snapshot') }}
    WHERE order_date <= '{{ snapshot_date }}'::date
    GROUP BY seller_id
)

SELECT
    seller_id,

    -- Use the macro for tier classification!
    {{ classify_tier('total_orders', 'volume') }} as seller_tier,

    total_orders,
    unique_products,
    total_revenue,
    avg_order_value,
    first_sale_date,
    last_sale_date,
    updated_at
FROM seller_metrics</pre>

            <h4>Step 3: Create the customer tiers model</h4>
            <p><strong>Create this new file:</strong> <code>models/intermediate/int_customer_tiers.sql</code></p>
            <pre>
{{ config(materialized='table') }}

WITH customer_metrics AS (
    SELECT
        o.customer_id,
        COUNT(DISTINCT o.order_id) as total_orders,
        SUM(p.payment_value) as total_revenue,
        AVG(p.payment_value) as avg_order_value,
        MIN(DATE(o.order_purchase_timestamp)) as first_order_date,
        MAX(DATE(o.order_purchase_timestamp)) as last_order_date
    FROM {{ ref('stg_orders') }} o
    INNER JOIN {{ ref('stg_order_payments') }} p
        ON o.order_id = p.order_id
    WHERE o.order_status NOT IN ('canceled', 'unavailable')
    GROUP BY o.customer_id
)

SELECT
    customer_id,

    -- Use the SAME macro for customer tiers!
    {{ classify_tier('total_orders', 'volume') }} as tier_by_volume,
    {{ classify_tier('total_revenue', 'revenue') }} as tier_by_revenue,

    total_orders,
    total_revenue,
    avg_order_value,
    first_order_date,
    last_order_date,

    -- Composite tier: take the better of the two
    CASE
        WHEN {{ classify_tier('total_orders', 'volume') }} = 'platinum'
          OR {{ classify_tier('total_revenue', 'revenue') }} = 'platinum' THEN 'platinum'
        WHEN {{ classify_tier('total_orders', 'volume') }} = 'gold'
          OR {{ classify_tier('total_revenue', 'revenue') }} = 'gold' THEN 'gold'
        WHEN {{ classify_tier('total_orders', 'volume') }} = 'silver'
          OR {{ classify_tier('total_revenue', 'revenue') }} = 'silver' THEN 'silver'
        ELSE 'bronze'
    END as composite_tier

FROM customer_metrics</pre>

            <h4>Step 4: Test the macro reusability</h4>
            <pre>
# Run the refactored seller model
dbt run --select int_seller_performance

# Run the new customer tiers model
dbt run --select int_customer_tiers

# Verify tier distribution for sellers
SELECT seller_tier, COUNT(*) as count
FROM int_seller_performance
GROUP BY seller_tier
ORDER BY
    CASE seller_tier
        WHEN 'platinum' THEN 1
        WHEN 'gold' THEN 2
        WHEN 'silver' THEN 3
        ELSE 4
    END;

# Verify tier distribution for customers
SELECT
    composite_tier,
    COUNT(*) as customer_count,
    AVG(total_orders) as avg_orders,
    AVG(total_revenue) as avg_revenue
FROM int_customer_tiers
GROUP BY composite_tier
ORDER BY
    CASE composite_tier
        WHEN 'platinum' THEN 1
        WHEN 'gold' THEN 2
        WHEN 'silver' THEN 3
        ELSE 4
    END;</pre>

            <div class="info-box">
                <h3>‚ú® What You've Achieved:</h3>
                <ul>
                    <li><strong>DRY Code:</strong> One macro, multiple uses</li>
                    <li><strong>Consistency:</strong> Sellers and customers use identical tier logic</li>
                    <li><strong>Flexibility:</strong> Easy to add new tier types (e.g., 'engagement', 'potential')</li>
                    <li><strong>Maintainability:</strong> Change tier thresholds in one place</li>
                </ul>
            </div>
        </div>

        <div class="optional-challenge">
            <h3>üåü Optional Challenge: Dynamic Tier Thresholds</h3>
            <p>Extend your macro to accept custom tier thresholds as parameters instead of hardcoding them.</p>

            <p><strong>The Idea:</strong> Right now, the thresholds (500, 100, 20) are hardcoded in the macro. What if you want different thresholds for different markets or time periods?</p>

            <p><strong>How to implement:</strong> Modify your <code>classify_tier</code> macro definition to accept threshold parameters (e.g., <code>platinum_threshold</code>, <code>gold_threshold</code>, <code>silver_threshold</code>) with default values.</p>

            <p><strong>Potential use cases:</strong></p>
            <ul>
                <li>Regional variations (different thresholds for US vs Brazil markets)</li>
                <li>Product categories (electronics vs groceries have different patterns)</li>
                <li>Testing different tier breakpoints to optimize business segments</li>
            </ul>

            <p><strong>Hint:</strong> Look at how macros can accept optional parameters with default values in the dbt documentation.</p>
        </div>
    </div>

    <h2>ü™ù Production Hooks - Automating Data Quality Monitoring</h2>

    <div class="info-box">
        <h3>Understanding Hooks</h3>
        <p><strong>What are Hooks?</strong> SQL statements that run automatically before or after your model builds.</p>

        <p><strong>Two types of hooks:</strong></p>
        <ul>
            <li><strong>Pre-hooks:</strong> Run BEFORE your model SQL (setup, cleanup)</li>
            <li><strong>Post-hooks:</strong> Run AFTER your model SQL (monitoring, indexing)</li>
        </ul>

        <p><strong>Why use hooks?</strong> Automate repetitive tasks like logging, monitoring, and optimization.</p>
    </div>

    <!-- Challenge 3: Production Hooks -->
    <div class="challenge-box">
        <h2>üéØ Challenge 3: Add Data Quality Monitoring Hook</h2>
        <p>Create a simple post-hook that automatically counts rows and null order_ids every time your model runs.</p>

        <div class="info-box">
            <h3>üìä What You'll Build</h3>
            <p>A post-hook that logs two simple metrics:</p>
            <ul>
                <li><strong>Total rows</strong> in the table</li>
                <li><strong>Null order_ids</strong> count</li>
            </ul>

            <h4>Why This Matters:</h4>
            <ul>
                <li><strong>Automatic Monitoring:</strong> Track data quality without manual queries</li>
                <li><strong>Historical Tracking:</strong> See trends over time</li>
                <li><strong>Early Warning:</strong> Detect when nulls appear</li>
            </ul>
        </div>

        <div class="info-box">
            <h3>üìã Setup: Create the Monitoring Table</h3>
            <p><strong>First, run this SQL to create the monitoring infrastructure:</strong></p>
            <pre>
-- Create monitoring schema
CREATE SCHEMA IF NOT EXISTS dbt_monitoring;

-- Create simple quality log table
CREATE TABLE IF NOT EXISTS dbt_monitoring.quality_log (
    log_id SERIAL PRIMARY KEY,
    checked_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    total_rows INTEGER,
    null_order_ids INTEGER
);

-- Verify it was created
SELECT * FROM dbt_monitoring.quality_log;</pre>

            <p><strong>Understanding the table:</strong></p>
            <ul>
                <li><code>log_id</code> - Auto-incrementing ID for each log entry</li>
                <li><code>checked_at</code> - When the check ran (auto-filled)</li>
                <li><code>total_rows</code> - Total number of rows in stg_orders</li>
                <li><code>null_order_ids</code> - How many rows have NULL order_id</li>
            </ul>
        </div>

        <h3>Requirements:</h3>
        <ol>
            <li>Add a post-hook to <code>stg_orders</code> that inserts a row into <code>dbt_monitoring.quality_log</code></li>
            <li>The hook should count:
                <ul>
                    <li>Total rows using <code>COUNT(*)</code></li>
                    <li>Null order_ids using <code>SUM(CASE WHEN order_id IS NULL THEN 1 ELSE 0 END)</code></li>
                </ul>
            </li>
            <li>Test by running <code>dbt run --select stg_orders</code> and checking for new rows in the log table</li>
        </ol>

        <button class="solution-button" onclick="toggleSolution('solution3')">Show Solution</button>
        <div id="solution3" class="solution">
            <h4>Add post-hook to stg_orders</h4>
            <p><strong>Edit models/staging/stg_orders.sql</strong> - add the config at the top:</p>
            <pre>
{{ config(
    materialized='table',
    post_hook=[
        "INSERT INTO dbt_monitoring.quality_log (total_rows, null_order_ids)
         SELECT
           COUNT(*) as total_rows,
           SUM(CASE WHEN order_id IS NULL THEN 1 ELSE 0 END) as null_order_ids
         FROM {{ this }}"
    ]
) }}

SELECT
    order_id,
    customer_id,
    order_status,
    order_purchase_timestamp::timestamp as order_purchase_timestamp,
    order_approved_at::timestamp as order_approved_at,
    order_delivered_carrier_date::timestamp as order_delivered_carrier_date,
    order_delivered_customer_date::timestamp as order_delivered_customer_date,
    order_estimated_delivery_date::timestamp as order_estimated_delivery_date,
    DATE(order_purchase_timestamp) as order_date
FROM {{ source('olist_data', 'olist_orders') }}
WHERE order_status != 'unavailable'
  AND order_purchase_timestamp IS NOT NULL</pre>

            <h4>Key Points:</h4>
            <ul>
                <li><strong>{{ this }}</strong> - refers to the table you just built (stg_orders)</li>
                <li><strong>CASE WHEN</strong> - counts rows where order_id IS NULL</li>
                <li><strong>checked_at</strong> - automatically filled by the database (DEFAULT CURRENT_TIMESTAMP)</li>
            </ul>

            <h4>Test the hook</h4>
            <pre>
# Run the model - the hook executes automatically after the model builds
dbt run --select stg_orders

# Check the log table for new entries
SELECT * FROM dbt_monitoring.quality_log ORDER BY checked_at DESC LIMIT 5;</pre>

            <h4>What You Should See:</h4>
            <p>Every time you run <code>stg_orders</code>, a new row is added:</p>
            <pre>
 log_id |      checked_at         | total_rows | null_order_ids
--------+-------------------------+------------+----------------
      2 | 2025-01-15 10:32:15.456 |      99441 |              0
      1 | 2025-01-15 10:30:45.123 |      99441 |              0
(2 rows)</pre>

            <p><strong>Success!</strong> Each run creates a new log entry - that's historical tracking in action.</p>

            <div class="info-box">
                <h3>‚ú® What You've Achieved:</h3>
                <ul>
                    <li><strong>Automatic Quality Checks:</strong> Every run logs data quality</li>
                    <li><strong>Zero Manual Work:</strong> Happens automatically on every dbt run</li>
                    <li><strong>Historical Tracking:</strong> Build up quality metrics over time</li>
                    <li><strong>Foundation for Alerts:</strong> Can add thresholds and notifications later</li>
                </ul>
            </div>
        </div>

        <div class="optional-challenge">
            <h3>üåü Optional Challenge: Track More Models</h3>
            <p>Extend your monitoring to other staging models:</p>

            <h4>Ideas to Implement:</h4>
            <ul>
                <li><strong>Add model_name column:</strong> Modify the table to include which model was checked</li>
                <li><strong>Monitor stg_customers:</strong> Add the same hook, track null customer_ids</li>
                <li><strong>Monitor stg_order_payments:</strong> Track null payment_values</li>
                <li><strong>Row count alerts:</strong> Add a second hook that compares current total_rows to the previous run and raises an error if it drops by > 10%</li>
            </ul>
        </div>
    </div>

    <h2>üß™ Understanding dbt Tests</h2>

    <div class="info-box">
        <h3>Testing Progression: Simple to Advanced</h3>

        <h4>1. Schema Tests (Start Here)</h4>
        <p>Pre-built tests applied via YAML configuration - the foundation of data quality:</p>
        <ul>
            <li><code>unique</code> - Ensures column has no duplicates</li>
            <li><code>not_null</code> - Ensures column has no nulls</li>
            <li><code>accepted_values</code> - Validates against a list</li>
            <li><code>relationships</code> - Checks foreign key integrity</li>
        </ul>

        <h4>2. Unit Tests (New in dbt 1.8+)</h4>
        <p>Test model logic with mock inputs and expected outputs - like unit tests in programming.</p>

        <h4>3. Custom Tests (Advanced)</h4>
        <p>Custom SQL queries in the <code>tests/</code> folder that return rows when they fail - for complex business logic validation.</p>

        <h4>Running Tests:</h4>
        <pre>
dbt test                           # Run all tests
dbt test --select model_name       # Test specific model
dbt test --select tag:critical     # Run tagged tests
dbt test --store-failures          # Save failures for debugging</pre>
    </div>

    <!-- Challenge 4: Schema Tests -->
    <div class="challenge-box">
        <h2>üéØ Challenge 4: Schema Tests Foundation</h2>
        <p>Start with schema tests - the foundation of data quality validation.</p>

        <h3>Requirements:</h3>
        <ol>
            <li>Create comprehensive schema tests for <code>stg_orders</code> and <code>stg_customers</code></li>
            <li>Test primary keys, foreign keys, and business rules</li>
            <li>Add data range validations</li>
            <li>Include severity levels (warn vs error)</li>
        </ol>

        <button class="solution-button" onclick="toggleSolution('solution4')">Show Solution</button>
        <div id="solution4" class="solution">
            <h4>models/staging/schema.yml</h4>
            <pre>
version: 2

models:
  - name: stg_orders
    description: "Cleaned orders from Olist dataset"
    config:
      tags: ["staging", "critical"]
    
    columns:
      - name: order_id
        description: "Unique order identifier"
        tests:
          - unique
          - not_null
      
      - name: customer_id
        description: "Customer who placed the order"
        tests:
          - not_null
          - relationships:
              to: ref('stg_customers')
              field: customer_id
              config:
                severity: error  # Critical business rule
      
      - name: order_status
        description: "Current order status"
        tests:
          - not_null
          - accepted_values:
              values: ['delivered', 'shipped', 'processing', 'canceled', 'invoiced', 'approved', 'created']
              config:
                severity: warn  # New statuses might appear
      
      - name: order_purchase_timestamp
        description: "When order was placed"
        tests:
          - not_null
          # SQLite compatible date range tests
          - dbt_utils.expression_is_true:
              expression: "date(order_purchase_timestamp) >= '2016-01-01'"
              config:
                severity: error
          - dbt_utils.expression_is_true:
              expression: "date(order_purchase_timestamp) <= date('now')"
              config:
                severity: error
      
      - name: order_date
        description: "Date of order (for partitioning)"
        tests:
          - not_null

  - name: stg_customers
    description: "Clean customer data"
    config:
      tags: ["staging", "reference"]
    
    columns:
      - name: customer_id
        description: "Primary key"
        tests:
          - unique
          - not_null
      
      - name: customer_city
        description: "Customer city"
        tests:
          - not_null
      
      - name: customer_state
        description: "Customer state (should be valid Brazilian states)"
        tests:
          - not_null
          - accepted_values:
              values: ['SP', 'RJ', 'MG', 'RS', 'PR', 'SC', 'BA', 'GO', 'ES', 'PE', 'CE', 'PA', 'DF', 'MT', 'MS', 'PB', 'RN', 'AL', 'PI', 'SE', 'RO', 'AC', 'AM', 'RR', 'AP', 'TO', 'MA']
              config:
                severity: warn  # New states are rare but possible

  - name: stg_order_payments
    description: "Payment information"
    columns:
      - name: order_id
        tests:
          - not_null
          - relationships:
              to: ref('stg_orders')
              field: order_id
      
      - name: payment_value
        description: "Payment amount"
        tests:
          - not_null
          - dbt_utils.expression_is_true:
              expression: ">= 0"
              config:
                severity: error
                error_if: "> 10"  # Fail if more than 10 negative values</pre>
            
            <p><strong>Run schema tests:</strong></p>
            <pre>
# Test specific models
dbt test --select stg_orders

# Test with different severities
dbt test --select stg_customers --fail-fast

# Store failures for inspection
dbt test --select staging --store-failures

# Check what failed
# SELECT * FROM main.dbt_test__audit_[test_name];</pre>
        </div>
        
        <div class="optional-challenge">
            <h3>üåü Optional Challenge: great-expectations Integration</h3>
            <p>Add advanced data quality tests using the dbt-expectations package:</p>
            
            <div class="info-box">
                <h4>üì¶ dbt-expectations Package</h4>
                <p>Provides 50+ additional tests inspired by Great Expectations framework:</p>
                <ul>
                    <li><strong>Statistical tests:</strong> expect_column_mean_to_be_between</li>
                    <li><strong>Distribution tests:</strong> expect_column_values_to_be_in_set</li>
                    <li><strong>Relationship tests:</strong> expect_table_row_count_to_equal_other_table</li>
                    <li><strong>Pattern tests:</strong> expect_column_values_to_match_regex</li>
                </ul>
                <p><strong>Installation:</strong> Add to packages.yml and run <code>dbt deps</code></p>
            </div>
            
            <button class="solution-button" onclick="toggleSolution('optional4')">Show Solution</button>
            <div id="optional4" class="solution">
                <h4>packages.yml</h4>
                <pre>
packages:
  - package: dbt-labs/dbt_utils
    version: 1.1.1
  - package: calogica/dbt_expectations
    version: 0.10.1</pre>
                
                <h4>Extended schema tests with dbt-expectations:</h4>
                <pre>
# Add to models/staging/schema.yml under stg_orders columns
      - name: order_purchase_timestamp
        tests:
          # Statistical validation
          - dbt_expectations.expect_column_values_to_be_between:
              min_value: 0
              max_value: 1000000  # Reasonable timestamp range
              
      - name: customer_id
        tests:
          # Format validation
          - dbt_expectations.expect_column_values_to_match_regex:
              regex: "^[a-fA-F0-9]{32}$"  # UUID format
              
# Table-level tests
  - name: stg_orders
    tests:
      # Row count validation
      - dbt_expectations.expect_table_row_count_to_be_between:
          min_value: 95000
          max_value: 105000
          
      # Completeness validation  
      - dbt_expectations.expect_table_column_count_to_equal:
          value: 9</pre>
                
                <p><strong>Run extended tests:</strong></p>
                <pre>
# Install packages first
dbt deps

# Run all tests including expectations
dbt test --select stg_orders</pre>
            </div>
        </div>
    </div>

    <!-- Challenge 5: Unit Tests -->
    <div class="challenge-box">
        <h2>üéØ Challenge 5: Unit Tests for Model Logic</h2>
        <p>Create unit tests to validate your model transformation logic with mock data.</p>
        
        <div class="info-box">
            <h3>üî¨ Unit Tests Explained</h3>
            <p>Unit tests work like unit tests in programming:</p>
            <ul>
                <li><strong>Given:</strong> Mock input data (what goes into your model)</li>
                <li><strong>When:</strong> Your model runs (automatic)</li>
                <li><strong>Then:</strong> Expected output data (what should come out)</li>
            </ul>
            <p>Perfect for testing business logic, edge cases, and transformations.</p>
        </div>
        
        <h3>Requirements:</h3>
        <ol>
            <li>Create a unit test for <code>stg_orders</code> date casting and filtering</li>
            <li>Test that unavailable orders are properly filtered out</li>
            <li>Validate that null timestamps are handled correctly</li>
            <li>Test the order_date derivation logic</li>
        </ol>
        
        <button class="solution-button" onclick="toggleSolution('solution5')">Show Solution</button>
        <div id="solution5" class="solution">
            <h4>tests/unit/test_stg_orders.yml</h4>
            <pre>
# Unit test for stg_orders transformation logic
version: 2

unit_tests:
  - name: test_stg_orders_filtering_and_dates
    description: "Test that orders are properly filtered and dates correctly derived"
    model: stg_orders
    
    given:
      - input: source('olist_data', 'olist_orders_dataset')
        rows:
          # Valid order - should appear in output
          - {order_id: "valid_001", customer_id: "customer_001", order_status: "delivered", 
             order_purchase_timestamp: "2018-01-15 10:30:00", order_approved_at: "2018-01-15 11:00:00"}
             
          # Unavailable order - should be filtered out
          - {order_id: "invalid_001", customer_id: "customer_002", order_status: "unavailable", 
             order_purchase_timestamp: "2018-01-16 11:00:00", order_approved_at: "2018-01-16 11:30:00"}
             
          # Null timestamp - should be filtered out
          - {order_id: "invalid_002", customer_id: "customer_003", order_status: "shipped", 
             order_purchase_timestamp: null, order_approved_at: "2018-01-17 12:00:00"}
             
          # Another valid order - should appear
          - {order_id: "valid_002", customer_id: "customer_004", order_status: "processing", 
             order_purchase_timestamp: "2018-01-20 14:15:30", order_approved_at: null}
    
    expect:
      rows:
        # Only valid orders should appear with proper date casting
        - {order_id: "valid_001", customer_id: "customer_001", order_status: "delivered",
           order_purchase_timestamp: "2018-01-15 10:30:00", order_date: "2018-01-15"}
        - {order_id: "valid_002", customer_id: "customer_004", order_status: "processing",
           order_purchase_timestamp: "2018-01-20 14:15:30", order_date: "2018-01-20"}

  - name: test_stg_orders_edge_cases
    description: "Test edge cases in order processing"
    model: stg_orders
    
    given:
      - input: source('olist_data', 'olist_orders_dataset')
        rows:
          # Edge case: very old order
          - {order_id: "old_001", customer_id: "customer_old", order_status: "delivered", 
             order_purchase_timestamp: "2015-12-31 23:59:59", order_approved_at: "2016-01-01 00:30:00"}
             
          # Edge case: future date (should still be included if not null)
          - {order_id: "future_001", customer_id: "customer_future", order_status: "created", 
             order_purchase_timestamp: "2025-01-01 10:00:00", order_approved_at: null}
    
    expect:
      rows:
        - {order_id: "old_001", customer_id: "customer_old", order_status: "delivered",
           order_purchase_timestamp: "2015-12-31 23:59:59", order_date: "2015-12-31"}
        - {order_id: "future_001", customer_id: "customer_future", order_status: "created",
           order_purchase_timestamp: "2025-01-01 10:00:00", order_date: "2025-01-01"}</pre>
           
            <p><strong>Run unit tests:</strong></p>
            <pre>
# Run specific unit test
dbt test --select test_type:unit

# Run unit test for specific model
dbt test --select stg_orders --select test_type:unit

# Get detailed output
dbt test --select test_stg_orders_filtering_and_dates --verbose</pre>
            
            <div class="warning-box">
                <strong>Unit Test Tips:</strong>
                <ul>
                    <li>Keep test data small and focused on specific logic</li>
                    <li>Test edge cases (nulls, empty strings, extreme values)</li>
                    <li>One test per logical concept</li>
                    <li>Use descriptive test names</li>
                </ul>
            </div>
        </div>
        
        <div class="optional-challenge">
            <h3>üåü Optional Challenge: Unit Test for Incremental Model</h3>
            <p>Test your incremental model <code>int_customer_daily_features_inc</code> from Session 2. Incremental model tests are different because you need to test BOTH the initial load AND incremental behavior.</p>

            <h4>Key Differences for Incremental Unit Tests:</h4>
            <ul>
                <li><strong>Test Initial Load:</strong> Verify behavior when table doesn't exist (full refresh)</li>
                <li><strong>Test Incremental Logic:</strong> Mock existing table data and verify only new rows are processed</li>
                <li><strong>Test Lookback Windows:</strong> Ensure incremental runs include proper lookback period</li>
                <li><strong>Verify is_incremental():</strong> Confirm the conditional logic works correctly</li>
            </ul>

            <h4>How to Test Incremental Models:</h4>
            <p>In your unit test, you can provide the model's previous state using the <code>this</code> input:</p>
            <pre>
unit_tests:
  - name: test_incremental_behavior
    model: int_customer_daily_features_inc
    given:
      # Mock the EXISTING table state (what's already in {{ this }})
      - input: this
        rows:
          - {customer_id: "c1", date: "2017-01-01", total_orders: 1}
          - {customer_id: "c1", date: "2017-01-02", total_orders: 1}

      # Provide new source data
      - input: ref('stg_orders')
        rows:
          - {customer_id: "c1", order_id: "o_new", order_purchase_timestamp: "2017-01-03 10:00:00"}

    expect:
      # Verify incremental run only processes new date
      rows:
        - {customer_id: "c1", date: "2017-01-03", total_orders: 2}</pre>

            <p><strong>Try creating a unit test that:</strong></p>
            <ul>
                <li>Mocks existing data in the incremental table (using <code>input: this</code>)</li>
                <li>Provides new orders in <code>stg_orders</code></li>
                <li>Verifies the lookback window works (processes last 7 days even if incremental)</li>
                <li>Confirms new features are calculated correctly</li>
            </ul>
        </div>
    </div>

    <!-- Challenge 6: Freshness Testing (OPTIONAL) -->
    <div class="challenge-box">
        <h2>üéØ Challenge 6 (Optional): Data Freshness Monitoring</h2>
        <p>Set up freshness tests to ensure your source data is up-to-date for production ML pipelines.</p>

        <div class="info-box">
            <h3>‚ö†Ô∏è Why This Challenge is Optional</h3>
            <p><strong>Realistic Testing Limitation:</strong> In our workshop environment, data doesn't naturally "flow" - our source tables (<code>olist_orders</code>, <code>olist_order_items</code>) are static historical datasets from 2016-2018.</p>

            <p><strong>What freshness tests check:</strong> They verify that data was loaded recently by comparing the <code>loaded_at</code> timestamp or a natural timestamp column (like <code>order_purchase_timestamp</code>) against the current time.</p>

            <p><strong>The challenge:</strong> Since our data is years old, freshness tests will always fail unless you manually insert new test data.</p>

            <h4>How to Test This (If You Want To):</h4>
            <ol>
                <li>Configure freshness checks as shown in the solution</li>
                <li>Run <code>dbt source freshness</code> - it will fail (data is old!)</li>
                <li><strong>Optional:</strong> Insert a new test order with today's timestamp to make the test pass</li>
                <li>Re-run <code>dbt source freshness</code> to see it succeed</li>
            </ol>

            <p><strong>In production:</strong> Freshness tests are critical! They detect when data pipelines break and alert you before downstream models fail.</p>
        </div>
        
        <div class="info-box">
            <h3>üìä Which Tables to Test & Thresholds</h3>
            <p><strong>Critical Tables with Natural Timestamps:</strong></p>
            <ul>
                <li><strong>olist_orders:</strong> Warn: 2 hours, Error: 4 hours (business critical, uses order_purchase_timestamp)</li>
                <li><strong>olist_order_items:</strong> Warn: 6 hours, Error: 12 hours (moderately important, uses shipping_limit_date)</li>
            </ul>
            <p><strong>Note:</strong> We're only monitoring tables with natural timestamp columns. Tables like customers and order_payments don't have reliable timestamps for freshness checks.</p>
        </div>
        
        <h3>Requirements:</h3>
        <ol>
            <li>Configure freshness checks for the 2 main transactional tables</li>
            <li>Set appropriate warning and error thresholds as specified above</li>
            <li>Run initial freshness test (should show errors - data is from 2018!)</li>
            <li>Insert new test data and verify freshness tests pass</li>
        </ol>

        <button class="solution-button" onclick="toggleSolution('solution6')">Show Solution</button>
        <div id="solution6" class="solution">
            <h4>models/staging/sources.yml</h4>
            <pre>
version: 2

sources:
  - name: olist_data
    description: "Raw Olist e-commerce data"
    
    tables:
      - name: olist_orders
        description: "Raw orders data - CRITICAL for ML features"
        loaded_at_field: order_purchase_timestamp
        
        # Strict thresholds for business-critical data
        freshness:
          warn_after: {count: 2, period: hour}
          error_after: {count: 4, period: hour}
        
        columns:
          - name: order_id
            description: "Primary key"
          - name: order_purchase_timestamp
            description: "Used for freshness monitoring"
      
      - name: olist_order_items
        description: "Order line items - moderately important"
        loaded_at_field: shipping_limit_date
        
        freshness:
          warn_after: {count: 6, period: hour}
          error_after: {count: 12, period: hour}
        
        columns:
          - name: order_id
            description: "Foreign key to orders"
          - name: shipping_limit_date
            description: "Used for freshness monitoring"</pre>
            
            <h4>Step-by-Step Testing Process:</h4>
            <pre>
# Step 1: Run initial freshness check (should show ERRORS)
dbt source freshness

# Expected output:
# ‚ùå ERROR: olist_orders is stale (last data from 2018)
# ‚ùå ERROR: olist_order_items is stale (last data from 2018)</pre>
            
            <div class="instructor-note">
                <h4>üéì Instructor Demo: Insert Fresh Data</h4>
                <p><strong>Run this script to simulate fresh data arrival:</strong></p>
                <pre>
-- Insert recent orders to make freshness tests pass
INSERT INTO olist_data.olist_orders (
    order_id, customer_id, order_status, 
    order_purchase_timestamp, order_approved_at
) VALUES 
    ('FRESH_001', 'TEST_CUSTOMER_001', 'processing', 
     datetime('now', '-30 minutes'), datetime('now', '-25 minutes')),
    ('FRESH_002', 'TEST_CUSTOMER_002', 'approved', 
     datetime('now', '-45 minutes'), datetime('now', '-40 minutes')),
    ('FRESH_003', 'TEST_CUSTOMER_003', 'shipped', 
     datetime('now', '-1 hours'), datetime('now', '-55 minutes'));

-- Insert corresponding order items
INSERT INTO olist_data.olist_order_items (
    order_id, order_item_id, product_id, seller_id,
    shipping_limit_date, price, freight_value
) VALUES 
    ('FRESH_001', 1, 'PROD_001', 'SELLER_001', 
     datetime('now', '+2 days'), 99.90, 15.50),
    ('FRESH_002', 1, 'PROD_002', 'SELLER_002', 
     datetime('now', '+3 days'), 45.00, 8.75);

-- Verify the data was inserted
SELECT order_id, order_status, order_purchase_timestamp 
FROM olist_data.olist_orders 
WHERE order_id LIKE 'FRESH_%'
ORDER BY order_purchase_timestamp DESC;</pre>
            </div>
            
            <h4>Step 2: Re-run freshness tests (should PASS now):</h4>
            <pre>
# Run freshness check again
dbt source freshness

# Expected output now:
# ‚úÖ PASS: olist_orders (30 minutes old, threshold: 2 hours)
# ‚úÖ PASS: olist_order_items (30 minutes old, threshold: 6 hours)</pre>
            
            <h4>Step 3: Clean up test data:</h4>
            <pre>
-- Remove test data after demonstration
DELETE FROM olist_data.olist_orders 
WHERE order_id LIKE 'FRESH_%';

DELETE FROM olist_data.olist_order_items 
WHERE order_id LIKE 'FRESH_%';

-- Verify cleanup
SELECT COUNT(*) as remaining_test_records
FROM olist_data.olist_orders 
WHERE order_id LIKE 'FRESH_%';</pre>
        </div>
        
        <div class="optional-challenge">
            <h3>üåü Optional Challenge: Automated Freshness Monitoring</h3>
            <p>Set up automated freshness monitoring with alerting:</p>
            <ul>
                <li>Create a script to check freshness and send alerts</li>
                <li>Log freshness results to a monitoring table</li>
                <li>Set up different alert channels for warnings vs errors</li>
                <li>Create a dashboard view of data freshness over time</li>
            </ul>
            
            <button class="solution-button" onclick="toggleSolution('optional6')">Show Solution</button>
            <div id="optional6" class="solution">
                <h4>Create monitoring infrastructure:</h4>
                <pre>
-- Create freshness monitoring table
CREATE TABLE IF NOT EXISTS dbt_monitoring.freshness_log (
    check_id SERIAL PRIMARY KEY,
    source_name TEXT,
    table_name TEXT,
    checked_at TIMESTAMP,
    latest_data_timestamp TIMESTAMP,
    hours_stale REAL,
    status TEXT, -- 'PASS', 'WARN', 'ERROR'
    threshold_warn_hours INTEGER,
    threshold_error_hours INTEGER
);

-- Create monitoring view
CREATE VIEW IF NOT EXISTS dbt_monitoring.freshness_dashboard AS
SELECT 
    source_name || '.' || table_name as full_table_name,
    latest_data_timestamp,
    hours_stale,
    status,
    CASE 
        WHEN status = 'ERROR' THEN 'üî¥'
        WHEN status = 'WARN' THEN 'üü°'
        ELSE 'üü¢'
    END as status_icon,
    checked_at
FROM dbt_monitoring.freshness_log
WHERE check_id IN (
    SELECT MAX(check_id)
    FROM dbt_monitoring.freshness_log
    GROUP BY source_name, table_name
)
ORDER BY hours_stale DESC;</pre>
                
                <h4>Python script for automated monitoring:</h4>
                <pre>
import subprocess
import json
import sqlite3
from datetime import datetime

def check_and_alert():
    # Run dbt source freshness with JSON output
    result = subprocess.run(
        ['dbt', 'source', 'freshness', '--output', 'json'],
        capture_output=True, text=True
    )
    
    # Parse results
    freshness_data = json.loads(result.stdout)
    
    # Connect to database
    conn = sqlite3.connect('data.db')
    cursor = conn.cursor()
    
    alerts = []
    
    for source in freshness_data.get('results', []):
        status = source.get('status', 'unknown')
        hours_stale = source.get('max_loaded_at_time_ago_in_s', 0) / 3600
        
        # Log to monitoring table
        cursor.execute("""
            INSERT INTO dbt_monitoring.freshness_log 
            (source_name, table_name, checked_at, hours_stale, status)
            VALUES (?, ?, ?, ?, ?)
        """, (source['unique_id'], source['table'], datetime.now(), hours_stale, status))
        
        # Generate alerts
        if status == 'ERROR':
            alerts.append(f"üî¥ CRITICAL: {source['table']} is {hours_stale:.1f} hours stale")
        elif status == 'WARN':
            alerts.append(f"üü° WARNING: {source['table']} is {hours_stale:.1f} hours stale")
    
    conn.commit()
    conn.close()
    
    # Send alerts (implement your preferred method)
    if alerts:
        alert_message = "\n".join(alerts)
        print(f"FRESHNESS ALERT:\n{alert_message}")
        # Could integrate with Slack, email, PagerDuty, etc.

if __name__ == "__main__":
    check_and_alert()</pre>
            </div>
        </div>
    </div>


    <!-- Summary -->
    <div class="info-box">
        <h2>üéâ Session 3 Complete!</h2>
        
        <h3>What You've Built:</h3>
        <ul>
            <li>‚úÖ Comprehensive testing strategy (schema ‚Üí unit ‚Üí custom)</li>
            <li>‚úÖ Production freshness monitoring with clear thresholds</li>
            <li>‚úÖ Automated hooks for data cleaning and quality tracking</li>
            <li>‚úÖ Business logic as code using seeds</li>
            <li>‚úÖ Enriched mart table ready for ML models</li>
            <li>‚úÖ Monitoring infrastructure for production operations</li>
        </ul>
        
        <h3>Production-Ready Capabilities:</h3>
        <ul>
            <li><strong>Quality Assurance:</strong> Multiple layers of testing prevent bad data</li>
            <li><strong>Operational Monitoring:</strong> Automated tracking of data freshness and quality</li>
            <li><strong>Performance Optimization:</strong> Indexes and statistics for fast queries</li>
            <li><strong>Business Logic Management:</strong> Version-controlled rules and reference data</li>
            <li><strong>Observability:</strong> Comprehensive logging and metrics</li>
        </ul>
        
        <h3>Key Concepts Mastered:</h3>
        <ul>
            <li>Testing progression: schema ‚Üí unit ‚Üí custom</li>
            <li>Freshness monitoring with appropriate thresholds</li>
            <li>Production hooks for automation</li>
            <li>Seeds for business logic as code</li>
            <li>Data quality tracking and alerting</li>
            <li>Performance optimization techniques</li>
        </ul>
        
        <h3>Next Steps (Back to Presentation):</h3>
        <p>Now we'll return to the presentation for the grand finale:</p>
        <ul>
            <li>üöÄ Feast integration for real-time serving</li>
            <li>üèóÔ∏è Complete production architecture</li>
            <li>üìà Scaling to enterprise workloads</li>
            <li>üó∫Ô∏è Your 30-day implementation roadmap</li>
        </ul>
        
        <div class="feast-box">
            <h3>üéä Congratulations!</h3>
            <p>You've built the production-ready backbone that powers ML systems at companies like Netflix, Spotify, and Airbnb. You now have the skills to:</p>
            <ul>
                <li>Build reliable, scalable feature pipelines</li>
                <li>Prevent the most common ML failures</li>
                <li>Deploy with confidence to production</li>
                <li>Scale from thousands to millions of entities</li>
            </ul>
        </div>
    </div>
</body>
</html>