<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Session 2: Advanced Patterns - Hands-on Exercises</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #e74c3c;
            padding-bottom: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 30px;
            border-left: 4px solid #e74c3c;
            padding-left: 10px;
        }
        
        h3 {
            color: #7f8c8d;
            margin-top: 20px;
        }
        
        .info-box {
            background-color: #e8f4fd;
            border-left: 4px solid #3498db;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }
        
        .warning-box {
            background-color: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }
        
        .challenge-box {
            background-color: #fff;
            border: 2px solid #e74c3c;
            padding: 20px;
            margin: 30px 0;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        .optional-challenge {
            background-color: #ffeaa7;
            border: 2px dashed #fdcb6e;
            padding: 15px;
            margin: 15px 0;
            border-radius: 8px;
        }
        
        code {
            background-color: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
        }
        
        pre {
            background-color: #2c3e50;
            color: #ecf0f1;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            margin: 15px 0;
        }
        
        .solution {
            display: none;
            background-color: #e8f8f5;
            border: 2px solid #27ae60;
            padding: 15px;
            margin-top: 10px;
            border-radius: 5px;
        }
        
        .solution-button {
            background-color: #e74c3c;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 5px;
            cursor: pointer;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .solution-button:hover {
            background-color: #c0392b;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
            background-color: white;
        }
        
        th, td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        
        th {
            background-color: #e74c3c;
            color: white;
        }
        
        tr:nth-child(even) {
            background-color: #f2f2f2;
        }
        
        .highlight {
            background-color: #ffeb3b;
            padding: 2px 4px;
            border-radius: 3px;
        }
        
        .recap-box {
            background-color: #f0f3f4;
            border: 2px solid #34495e;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }
    </style>
    <script>
        function toggleSolution(id) {
            var solution = document.getElementById(id);
            var button = event.target;
            if (solution.style.display === 'none' || solution.style.display === '') {
                solution.style.display = 'block';
                button.textContent = 'Hide Solution';
            } else {
                solution.style.display = 'none';
                button.textContent = 'Show Solution';
            }
        }
    </script>
</head>
<body>
    <h1>üöÄ Session 2: Advanced Patterns - Hands-on Exercises</h1>
    
    <div class="info-box">
        <h3>‚è±Ô∏è Duration: 1-1.5 hours</h3>
        <p><strong>Prerequisites:</strong> Completed Session 1 models running successfully</p>
        <p><strong>Goal:</strong> Add advanced features: snapshots, incremental models, macros, and late-arriving data handling</p>
    </div>

    <div class="recap-box">
        <h2>üìã Quick Recap from Session 1</h2>
        <p><strong>What We Built:</strong></p>
        <ul>
            <li>‚úÖ Staging tables (<code>stg_orders</code>, <code>stg_customers</code>, etc.)</li>
            <li>‚úÖ Customer landing table (<code>int_customer_landing</code>)</li>
            <li>‚úÖ Daily feature table (<code>int_customer_daily_features</code>)</li>
            <li>‚úÖ Used sources, refs, and Jinja templating</li>
        </ul>
        <p><strong>Today's Focus:</strong> Make it production-ready with incremental processing and handle changing dimensions!</p>
    </div>

    <h2>üì∏ Slowly Changing Dimensions (SCD) Type 2</h2>
    
    <div class="info-box">
        <h3>Understanding SCD Type 2 with dbt Snapshots</h3>
        
        <p><strong>What are Slowly Changing Dimensions?</strong></p>
        <p>Customer attributes that change over time (payment preferences, locations, segments). We need to track history for point-in-time correctness.</p>
        
        <p><strong>dbt Snapshots Solution:</strong></p>
        <ul>
            <li>Automatically tracks changes in source tables</li>
            <li>Creates SCD Type 2 tables with validity periods</li>
            <li>Adds <code>dbt_valid_from</code> and <code>dbt_valid_to</code> columns</li>
        </ul>
        
        <p><strong>How it works:</strong></p>
        <pre>
-- First run: Takes snapshot of current state
-- Subsequent runs: Detects changes and creates new rows
-- Result: Complete history with validity periods

customer_id | favorite_payment | dbt_valid_from | dbt_valid_to
C001        | credit_card      | 2017-01-01     | 2018-03-15
C001        | boleto          | 2018-03-15     | NULL</pre>
    </div>

    <!-- Challenge 1: SCD Type 2 - SIMPLIFIED -->
    <div class="challenge-box">
        <h2>üéØ Challenge 1: Simple Snapshot Implementation</h2>
        <p>Create a basic snapshot of customer payment preferences based on existing intermediate models.</p>
        
        <h3>Requirements:</h3>
        <ol>
            <li>Create a simple intermediate table with customer payment preferences</li>
            <ul style="margin-left: 0.5em;">
                <li>Calculate each users most common payment type and save it in the table</li>
                <li>The end results should be a table with one row per user and the columns: customer_id, favorite_payment_method, usage_count, updated_at</li>
            </ul>
            <li>Set up a snapshot to track changes over time</li>
            <li>Test the snapshot functionality</li>
        </ol>
        
        <button class="solution-button" onclick="toggleSolution('solution1')">Show Solution</button>
        <div id="solution1" class="solution">
            <h4>Step 1: Create models/intermediate/int_customer_payment_preference.sql</h4>
            <pre>
{{ config(materialized='table') }}

WITH customer_payments AS (
    -- Get most common payment type per customer
    SELECT 
        o.customer_id,
        p.payment_type,
        COUNT(*) as payment_count
    FROM {{ ref('stg_orders') }} o
    INNER JOIN {{ ref('stg_order_payments') }} p
        ON o.order_id = p.order_id
    WHERE o.order_status NOT IN ('canceled', 'unavailable')
    GROUP BY 1, 2
),

ranked_preferences AS (
    SELECT 
        customer_id,
        payment_type,
        payment_count,
        ROW_NUMBER() OVER (
            PARTITION BY customer_id 
            ORDER BY payment_count DESC, payment_type
        ) as rank
    FROM customer_payments
)

SELECT 
    customer_id,
    payment_type as favorite_payment_method,
    payment_count as usage_count,
    CURRENT_TIMESTAMP::timestamp as updated_at  -- For snapshot tracking
FROM ranked_preferences
WHERE rank = 1</pre>
            
            <h4>Step 2: Run the model to create the table</h4>
            <pre>
# Create the intermediate table
dbt run --select int_customer_payment_preference
</pre>
            
            <h4>Step 3: Create snapshots/snap_payment_preference.sql</h4>
            <pre>
{% snapshot snap_payment_preference %}
    {{
        config(
          target_schema='snapshots',
          strategy='timestamp',
          unique_key='customer_id',
          updated_at='updated_at',
        )
    }}
    
    SELECT * FROM {{ ref('int_customer_payment_preference') }}
    
{% endsnapshot %}</pre>
            
            <h4>Step 4: Run the snapshot</h4>
            <pre>
# First run - creates initial snapshot
dbt snapshot --select snap_payment_preference

# Run again - no changes should be detected
dbt snapshot --select snap_payment_preference</pre>
            
            <p><strong>Verify in your database:</strong></p>
            <pre>
SELECT customer_id, favorite_payment_method, dbt_valid_from, dbt_valid_to
FROM snapshots.snap_payment_preference
LIMIT 10;</pre>
        </div>
        
        <div class="optional-challenge">
            <h3>üåü Optional Challenge: Advanced Preference Tracking</h3>
            <p>Create the more complex model from the original solution that tracks payment preferences over time with:</p>
            <ul>
                <li>Daily preference calculations</li>
                <li>Change detection logic</li>
                <li>Cumulative payment counting</li>
                <li>Preference change events</li>
            </ul>
        </div>
    </div>

    <h2>üîß Variables in dbt</h2>
    
    <div class="info-box">
        <h3>Understanding dbt Variables Hierarchy</h3>
        
        <p><strong>What are dbt Variables?</strong></p>
        <p>Variables allow you to pass values from outside into your models, making them configurable.</p>
        
        <p><strong>Variable Hierarchy (Bottom Wins):</strong></p>
        <ol>
            <li><strong>Project Level:</strong> <code>dbt_project.yml</code> vars section</li>
            <li><strong>Environment/Profile Level:</strong> <code>profiles.yml</code> vars</li>
            <li><strong>Command Line:</strong> <code>--vars</code> flag (highest priority)</li>
        </ol>
        
        <p><strong>Examples:</strong></p>
        <pre>
# 1. Project level (dbt_project.yml)
vars:
  lookback_days: [3, 7, 14]
  churn_threshold: 90

# 2. Command line (highest priority)
dbt run --vars '{lookback_days: [7, 14, 30], churn_threshold: 120}'

# 3. In model (with default fallback)
{% set lookback_days = var('lookback_days', [3, 7, 14]) %}
{% set threshold = var('churn_threshold', 90) %}</pre>
        
        <p><strong>Priority Order:</strong> Command Line > Profile > Project > Model Default</p>
    </div>

    <!-- Challenge 2: Variables - ENHANCED -->
    <div class="challenge-box">
        <h2>üéØ Challenge 2: Master Variable Hierarchy</h2>
        <p>Learn how dbt resolves variables from different sources and test the hierarchy in action.</p>
        
        <h3>Requirements:</h3>
        <ol>
            <li>Set up variables at project level</li>
            <li>Override with command line variables</li>
            <li>Update the daily features model to use variables</li>
            <li>Test variable resolution priority</li>
        </ol>
        
        <button class="solution-button" onclick="toggleSolution('solution2')">Show Solution</button>
        <div id="solution2" class="solution">
            <h4>Step 1: Add to dbt_project.yml (Project Level)</h4>
            <pre>
# Add this to your dbt_project.yml
vars:
  lookback_days: [3, 7, 14]
  max_date: '2018-10-31'
  churn_threshold_days: 90
  min_order_value: 10</pre>
            
            <h4>Step 2: Update models/intermediate/int_customer_daily_features.sql</h4>
            <pre>
{{ config(materialized='table') }}

-- Use variables with hierarchy demonstration
{% set lookback_days = var('lookback_days') %}
{% set max_date = var('max_date') %}
{% set min_order_value = var('min_order_value', 0) %}

-- Log variable values to see what's being used
{{ log("Using lookback_days: " ~ lookback_days, info=True) }}
{{ log("Max feature date: " ~ max_date, info=True) }}
{{ log("Min order value: " ~ min_order_value, info=True) }}

WITH customer_dates AS (
    SELECT 
        c.customer_id,
        c.landing_date,
        d.date_day as date
    FROM {{ ref('int_customer_landing') }} c
    CROSS JOIN (
        {{ dbt_utils.date_spine(
            datepart="day",
            start_date="'2016-01-01'::date",
            end_date="'2018-12-31'::date"
        ) }}
    ) d
    WHERE d.date_day >= c.landing_date
      AND d.date_day <= '{{ max_date }}'::date
),

daily_payments AS (
    SELECT
        o.customer_id,
        DATE(o.order_purchase_timestamp) as order_date,
        SUM(p.payment_value) as daily_payment_value,
        COUNT(DISTINCT o.order_id) as daily_order_count
    FROM {{ ref('stg_orders') }} o
    INNER JOIN {{ ref('stg_order_payments') }} p
        ON o.order_id = p.order_id
    WHERE o.order_status NOT IN ('canceled', 'unavailable')
      AND p.payment_value >= {{ min_order_value }}  -- Using variable
    GROUP BY 1, 2
),

features AS (
    SELECT
        cd.customer_id,
        cd.date,
        cd.landing_date,
        
        -- Cumulative payment value
        SUM(COALESCE(dp.daily_payment_value, 0)) OVER (
            PARTITION BY cd.customer_id 
            ORDER BY cd.date 
            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
        ) as total_payment_value,
        
        -- Dynamic rolling window features
        {% for days in lookback_days %}
        SUM(COALESCE(dp.daily_payment_value, 0)) OVER (
            PARTITION BY cd.customer_id 
            ORDER BY cd.date 
            ROWS BETWEEN {{ days - 1 }} PRECEDING AND CURRENT ROW
        ) as payment_{{ days }}d,
        
        SUM(COALESCE(dp.daily_order_count, 0)) OVER (
            PARTITION BY cd.customer_id 
            ORDER BY cd.date 
            ROWS BETWEEN {{ days - 1 }} PRECEDING AND CURRENT ROW
        ) as orders_{{ days }}d,
        {% endfor %}
        
        -- Days metrics
        cd.date - cd.landing_date as days_since_landing
        
    FROM customer_dates cd
    LEFT JOIN daily_payments dp
        ON cd.customer_id = dp.customer_id
        AND cd.date = dp.order_date
)

SELECT * FROM features</pre>
            
            <h4>Step 3: Test Variable Hierarchy</h4>
            <pre>
# Test 1: Use project-level variables (from dbt_project.yml)
dbt run --select int_customer_daily_features
# Check logs - should show [3, 7, 14] for lookback_days

# Test 2: Override with command line (highest priority)
dbt run --select int_customer_daily_features --vars '{lookback_days: [1, 5, 10]}'
# Check logs - should show [1, 5, 10] for lookback_days

# Test 3: Partial override
dbt run --select int_customer_daily_features --vars '{max_date: "2018-06-30"}'
# lookback_days should come from project, max_date from command line

# Test 4: See compiled SQL differences
dbt compile --select int_customer_daily_features --vars '{lookback_days: [30]}'
# Check target/compiled/.../int_customer_daily_features.sql</pre>
            
            <p><strong>Variable Resolution Test:</strong></p>
            <p>Watch the dbt logs during runs to see which variable values are actually being used!</p>
        </div>
        
        <div class="optional-challenge">
            <h3>üåü Optional Challenge: Environment-Specific Variables</h3>
            <p>Set up different variables for dev vs prod environments:</p>
            <ul>
                <li>Dev: Smaller lookback windows, limited date ranges</li>
                <li>Prod: Full date ranges, longer lookback windows</li>
                <li>Use <code>target.name</code> to conditionally set variables</li>
            </ul>
            
            <p><strong>Example:</strong></p>
            <pre>
-- In your model
{% if target.name == 'dev' %}
    {% set lookback_days = [3, 7] %}
{% else %}
    {% set lookback_days = [3, 7, 14, 30] %}
{% endif %}

-- Run with: dbt run --target dev  OR  dbt run --target prod</pre>
        </div>
    </div>

    <h2>üé≠ Macros in dbt</h2>
    
    <div class="info-box">
        <h3>Understanding Macros - You've Already Used Them!</h3>
        
        <p><strong>What are Macros?</strong></p>
        <p>Macros are reusable pieces of SQL code, like functions in programming languages.</p>
        
        <p><strong>You've Already Used These Macros from Session 1:</strong></p>
        <ul>
            <li><strong>dbt_utils.generate_surrogate_key():</strong> Created unique keys for customer landing</li>
            <li><strong>dbt_utils.date_spine():</strong> Generated date ranges for daily features</li>
            <li><strong>dbt_utils.pivot():</strong> Converted payment types to columns</li>
            <li><strong>Built-in {{ ref() }} and {{ source() }}:</strong> These are macros too!</li>
        </ul>
        
        <p><strong>Now you'll create your own:</strong></p>
        <pre>
-- macros/get_payment_metrics.sql
{% macro get_payment_metrics(days) %}
    SUM(CASE 
        WHEN order_date >= date - {{ days }} 
        THEN payment_value 
    END) as payment_{{ days }}d
{% endmacro %}</pre>
        
        <p><strong>Using your macro:</strong></p>
        <pre>
SELECT 
    customer_id,
    {{ get_payment_metrics(7) }},
    {{ get_payment_metrics(30) }}
FROM payments</pre>
        
        <p><strong>Macros vs Functions vs Variables:</strong></p>
        <ul>
            <li><strong>Variables:</strong> Store values (numbers, strings, lists)</li>
            <li><strong>Macros:</strong> Generate SQL code dynamically</li>
            <li><strong>Functions:</strong> SQL functions run in the database</li>
        </ul>
    </div>

    <!-- Challenge 3: Macros - SIMPLIFIED RFM -->
    <div class="challenge-box">
        <h2>üéØ Challenge 3: Create RFM Metrics Macro</h2>
        <p>Build a simple macro to calculate RFM (Recency, Frequency, Monetary) raw metrics - the foundation of customer analysis.</p>
        
        <div class="info-box">
            <h3>üìä What is RFM Analysis?</h3>
            <p><strong>RFM</strong> is a customer analysis technique that measures three key dimensions:</p>
            
            <h4>The Three Metrics:</h4>
            <ul>
                <li><strong>Recency (R):</strong> Days since the customer's last purchase (lower = more recent)</li>
                <li><strong>Frequency (F):</strong> Number of orders the customer has made</li>
                <li><strong>Monetary (M):</strong> Total amount the customer has spent</li>
            </ul>
            
            <h4>Business Value:</h4>
            <p>These metrics help identify your best customers, at-risk customers, and guide marketing strategies.</p>
        </div>
        
        <h3>Requirements:</h3>
        <ol>
            <li>Create macro <code>calculate_rfm_metrics()</code></li>
            <li>Accept parameters for analysis date and lookback period</li>
            <li>Return raw recency days, frequency count, and monetary value</li>
            <li>Use in a customer analysis model</li>
        </ol>
        
        <button class="solution-button" onclick="toggleSolution('solution3')">Show Solution</button>
        <div id="solution3" class="solution">
            <h4>macros/calculate_rfm_metrics.sql</h4>
            <pre>
{% macro calculate_rfm_metrics(analysis_date='2018-10-31', lookback_days=365) %}

SELECT 
    o.customer_id,
    -- Recency: Days since last purchase (lower is better)
    '{{ analysis_date }}'::date - MAX(DATE(o.order_purchase_timestamp)) as recency_days,
    
    -- Frequency: Number of distinct orders
    COUNT(DISTINCT o.order_id) as frequency_orders,
    
    -- Monetary: Total amount spent
    SUM(p.payment_value) as monetary_value,
    
    '{{ analysis_date }}'::date as analysis_date
    
FROM {{ ref('stg_orders') }} o
INNER JOIN {{ ref('stg_order_payments') }} p
    ON o.order_id = p.order_id
WHERE o.order_status NOT IN ('canceled', 'unavailable')
  AND DATE(o.order_purchase_timestamp) >= ('{{ analysis_date }}'::date - {{ lookback_days }})
  AND DATE(o.order_purchase_timestamp) <= '{{ analysis_date }}'::date
GROUP BY o.customer_id

{% endmacro %}</pre>
            
            <h4>Using the macro - Create models/intermediate/int_customer_rfm_metrics.sql</h4>
            <pre>
{{ config(materialized='table') }}

-- Use the RFM macro with different time periods
{{ calculate_rfm_metrics(
    analysis_date='2018-10-31',
    lookback_days=365
) }}</pre>
            
            <h4>Alternative usage with variables:</h4>
            <pre>
{{ config(materialized='table') }}

{% set analysis_date = var('rfm_analysis_date', '2018-10-31') %}
{% set lookback = var('rfm_lookback_days', 365) %}

{{ calculate_rfm_metrics(
    analysis_date=analysis_date,
    lookback_days=lookback
) }}</pre>
            
            <h4>Test the RFM metrics:</h4>
            <pre>
# Run with default parameters
dbt run --select int_customer_rfm_metrics

# Test with different analysis date
dbt run --select int_customer_rfm_metrics --vars '{rfm_analysis_date: "2018-06-30"}'

# Check the results in your database
# SELECT 
#     customer_id,
#     recency_days,
#     frequency_orders,
#     monetary_value
# FROM int_customer_rfm_metrics
# ORDER BY monetary_value DESC
# LIMIT 10;</pre>
            
            <p><strong>Business Value:</strong></p>
            <ul>
                <li><strong>Identify Best Customers:</strong> Low recency + high frequency + high monetary</li>
                <li><strong>Find At-Risk Customers:</strong> High recency days (haven't purchased recently)</li>
                <li><strong>Spot Big Spenders:</strong> Sort by monetary_value to find VIP customers</li>
                <li><strong>Track Engagement:</strong> Monitor frequency_orders over time</li>
            </ul>
        </div>
        
        <div class="optional-challenge">
            <h3>üåü Optional Challenge: Add RFM Scoring and Segmentation</h3>
            <p>Enhance the macro to add:</p>
            <ul>
                <li>Calculate quintile scores (1-5) for each metric using NTILE()</li>
                <li>Create customer segments (Champions, Loyal, At Risk, etc.)</li>
                <li>Add product category preferences per customer</li>
                <li>Calculate customer lifetime value (CLV)</li>
            </ul>
        </div>
    </div>

    <!-- Challenge 4: Build Control with Macros (MOVED UP) -->
    <div class="challenge-box">
        <h2>üéØ Challenge 4: Build Control Timestamps Macro</h2>
        <p>Create a macro that limits data processing to a specific timestamp - essential for testing incremental models properly.</p>
        
        <div class="info-box">
            <h3>üîß Why Build Control Matters for Testing</h3>
            <p>When testing incremental models, you need to simulate how they process data over time. This macro lets you:</p>
            <ul>
                <li><strong>Step 1:</strong> Build with data up to date X</li>
                <li><strong>Step 2:</strong> Build incrementally with data up to date Y</li>
                <li><strong>Step 3:</strong> Verify only new data (X to Y) was processed</li>
            </ul>
            <p>Without this control, you can't properly test incremental behavior!</p>
        </div>
        
        <h3>Requirements:</h3>
        <ol>
            <li>Create macro <code>build_until()</code> that accepts a timestamp column and cutoff date</li>
            <li>Use variables to pass the cutoff date</li>
            <li>Apply to staging models for testing</li>
            <li>Test incremental behavior step by step</li>
        </ol>
        
        <button class="solution-button" onclick="toggleSolution('solution4')">Show Solution</button>
        <div id="solution4" class="solution">
            <h4>macros/build_until.sql</h4>
            <pre>
{% macro build_until(column_name, default_date='2999-12-31') %}
    
    {% set build_until_date = var('build_until_date', default_date) %}
    
    {% if execute %}
        {{ log("üîß Build Control: Limiting " ~ column_name ~ " to " ~ build_until_date, info=True) }}
    {% endif %}
    
    AND {{ column_name }}::timestamp <= '{{ build_until_date }}'::timestamp
    
{% endmacro %}</pre>
            
            <h4>Apply to models/staging/stg_orders.sql (with build control)</h4>
            <pre>
{{ config(materialized='table') }}

SELECT
    order_id,
    customer_id,
    order_status,
    order_purchase_timestamp::timestamp as order_purchase_timestamp,
    order_approved_at::timestamp as order_approved_at,
    order_delivered_carrier_date::timestamp as order_delivered_carrier_date,
    order_delivered_customer_date::timestamp as order_delivered_customer_date,
    order_estimated_delivery_date::timestamp as order_estimated_delivery_date,
    DATE(order_purchase_timestamp) as order_date

FROM {{ source('olist_data', 'olist_orders_dataset') }}

WHERE order_status != 'unavailable'
  AND order_purchase_timestamp IS NOT NULL
  
  -- Apply build control macro
  {{ build_until('order_purchase_timestamp') }}</pre>
            
            <h4>Test the Build Control Macro:</h4>
            <pre>
# Step 1: Build with limited data (January 2017 only)
dbt run --select stg_orders --full-refresh --vars '{build_until_date: "2017-01-31"}'

# Verify in database:
# SELECT MIN(order_date), MAX(order_date), COUNT(*) FROM stg_orders;
# Should show dates only up to 2017-01-31

# Step 2: Extend the date range
dbt run --select stg_orders --full-refresh --vars '{build_until_date: "2017-02-28"}'

# Verify again - should now include February data

# Step 3: Remove limit to get all data
dbt run --select stg_orders --full-refresh

# Check compiled SQL to see the difference
cat target/compiled/your_project/models/staging/stg_orders.sql</pre>
            
            <div class="warning-box">
                <strong>Important for Testing:</strong> This macro is essential for testing incremental models properly. You'll use it in the next challenge to simulate real-world data arrival patterns.
            </div>
        </div>
    </div>

    <h2>üìà Incremental Models - The Production Game Changer</h2>
    
    <div class="info-box">
        <h3>Understanding Incremental Models from Session 2 Presentation</h3>
        
        <p><strong>Key Concepts from Today's Presentation:</strong></p>
        <ul>
            <li><strong>Problem:</strong> Processing 100M orders daily is expensive and slow</li>
            <li><strong>Solution:</strong> Only process new/changed data (10-100x faster)</li>
            <li><strong>Strategies:</strong> Append, Merge, Delete+Insert</li>
            <li><strong>Late Data:</strong> Use lookback windows or ingestion time tracking</li>
        </ul>
        
        <p><strong>How to Test Incremental Models Properly:</strong></p>
        <ol>
            <li><strong>Full Refresh + Limited Data:</strong> Build table with subset of data</li>
            <li><strong>Check Compiled SQL:</strong> Verify the CREATE TABLE statement</li>
            <li><strong>Full Refresh + More Data:</strong> Rebuild with more data using build control macro</li>
            <li><strong>Incremental Run:</strong> Run without --full-refresh</li>
            <li><strong>Check Incremental SQL:</strong> Verify the MERGE/INSERT statement</li>
            <li><strong>Validate Data:</strong> Ensure only new data was processed</li>
        </ol>
        
        <div class="warning-box">
            <strong>Live Demo Alert!</strong> During class, we'll demonstrate this step-by-step process live, showing you exactly how to inspect the compiled SQL and validate incremental behavior.
        </div>
    </div>

    <!-- Challenge 5: Incremental Models (MOVED DOWN) -->
    <div class="challenge-box">
        <h2>üéØ Challenge 5: Build and Test Incremental Models</h2>
        <p>Convert your feature models to incremental and learn to test them properly using the build control macro from Challenge 4.</p>
        
        <h3>Requirements:</h3>
        <ol>
            <li>Create a new incremental model <code>int_customer_daily_features_inc.sql</code> (similar to <code>int_customer_daily_features.sql</code> but with incremental logic)</li>
            <li>Use the build control macro for testing</li>
            <li>Follow the proper testing workflow</li>
            <li>Verify incremental behavior</li>
        </ol>
        
        <div class="info-box">
            <strong>üí° Reminder:</strong> The <code>{{ this }}</code> keyword in dbt refers to the current model's table in the database. When used inside an incremental model, it lets you query the existing table to find the maximum date or other metadata to determine what new data to process.
        </div>
        
        <button class="solution-button" onclick="toggleSolution('solution5')">Show Solution</button>
        <div id="solution5" class="solution">
            <h4>models/intermediate/int_customer_daily_features_inc.sql (Incremental Version)</h4>
            <pre>
{{ config(
    materialized='incremental',
    unique_key=['customer_id', 'date'],
    incremental_strategy='merge'
) }}

-- Use variables with hierarchy demonstration
{% set lookback_days = var('lookback_days', [3, 7, 14]) %}
{% set max_date = var('max_date', '2018-10-31') %}

WITH customer_dates AS (
    SELECT 
        c.customer_id,
        c.landing_date,
        d.date_day as date
    FROM {{ ref('int_customer_landing') }} c
    CROSS JOIN (
        {{ dbt_utils.date_spine(
            datepart="day",
            start_date="'2016-01-01'::date",
            end_date="'2018-12-31'::date"
        ) }}
    ) d
    WHERE d.date_day >= c.landing_date
      AND d.date_day <= '{{ max_date }}'::date
      
    {% if is_incremental() %}
      -- Only process recent feature dates for incremental runs
      AND d.date_day >= (
        SELECT MAX(date) - INTERVAL '7 days'
        FROM {{ this }}
      )
    {% endif %}
),

daily_payments AS (
    SELECT
        o.customer_id,
        DATE(o.order_purchase_timestamp) as order_date,
        SUM(p.payment_value) as daily_payment_value,
        COUNT(DISTINCT o.order_id) as daily_order_count
    FROM {{ ref('stg_orders') }} o
    INNER JOIN {{ ref('stg_order_payments') }} p
        ON o.order_id = p.order_id
    WHERE o.order_status NOT IN ('canceled', 'unavailable')
    
    {% if is_incremental() %}
      -- Look back for late-arriving data
      AND DATE(o.order_purchase_timestamp) >= (
        SELECT MAX(date) - INTERVAL '14 days'
        FROM {{ this }}
      )
    {% endif %}
    
    GROUP BY 1, 2
),

features AS (
    SELECT
        cd.customer_id,
        cd.date,
        cd.landing_date,
        
        -- Cumulative payment value
        SUM(COALESCE(dp.daily_payment_value, 0)) OVER (
            PARTITION BY cd.customer_id 
            ORDER BY cd.date 
            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
        ) as total_payment_value,
        
        -- Dynamic rolling window features
        {% for days in lookback_days %}
        SUM(COALESCE(dp.daily_payment_value, 0)) OVER (
            PARTITION BY cd.customer_id 
            ORDER BY cd.date 
            ROWS BETWEEN {{ days - 1 }} PRECEDING AND CURRENT ROW
        ) as payment_{{ days }}d,
        
        SUM(COALESCE(dp.daily_order_count, 0)) OVER (
            PARTITION BY cd.customer_id 
            ORDER BY cd.date 
            ROWS BETWEEN {{ days - 1 }} PRECEDING AND CURRENT ROW
        ) as orders_{{ days }}d,
        {% endfor %}
        
        -- Days metrics
        cd.date - cd.landing_date as days_since_landing
        
    FROM customer_dates cd
    LEFT JOIN daily_payments dp
        ON cd.customer_id = dp.customer_id
        AND cd.date = dp.order_date
)

SELECT * FROM features</pre>
            
            <h4>Proper Testing Workflow (Step by Step):</h4>
            <pre>
# STEP 1: Full refresh with limited data (January 2017)
dbt run --select int_customer_daily_features --full-refresh \
  --vars '{max_date: "2017-01-31"}'

# Check what was built:
# SELECT MIN(date), MAX(date), COUNT(*)
# FROM int_customer_daily_features;

# STEP 2: Check the compiled SQL (CREATE TABLE)
cat target/run/dbt_workshop/models/intermediate/int_customer_daily_features_inc.sql
# Should see CREATE TABLE statement, no is_incremental() logic

# STEP 3: Full refresh with more data (February 2017)
dbt run --select int_customer_daily_features --full-refresh \
  --vars '{max_date: "2017-02-28"}'

# Verify more data was added

# STEP 4: Now test incremental behavior (March 2017)
dbt run --select int_customer_daily_features \
  --vars '{max_date: "2017-03-31"}'

# STEP 5: Check the incremental SQL (MERGE statement)
cat target/run/your_project/models/intermediate/int_customer_daily_features.sql
# Should see MERGE statement with incremental logic

# STEP 6: Verify only new data was processed
# Count should increase, but only with March data</pre>
            
            <div class="info-box">
                <h3>üîç What to Look for in Compiled SQL:</h3>
                <p><strong>Full Refresh:</strong></p>
                <ul>
                    <li><code>CREATE TABLE</code> statement</li>
                    <li>No <code>is_incremental()</code> logic in WHERE clauses</li>
                    <li>Processes all data within date limits</li>
                </ul>
                
                <p><strong>Incremental Run:</strong></p>
                <ul>
                    <li><code>MERGE</code> or <code>INSERT</code> statement</li>
                    <li>Incremental logic in WHERE clauses</li>
                    <li>Only recent data processed</li>
                </ul>
            </div>
            
            <p><strong>Validate the Magic Happened:</strong></p>
            <pre>
# After incremental run, check the lookback logic worked:
# SELECT date, COUNT(*)
# FROM int_customer_daily_features
# WHERE date >= '2017-02-15'  -- Should see lookback data
# GROUP BY date
# ORDER BY date;
</pre>
        </div>
        
        <div class="optional-challenge">
            <h3>üåü Optional Challenge: Convert All Models to Incremental</h3>
            <p>Apply incremental strategies to:</p>
            <ul>
                <li>All staging models (with appropriate strategies)</li>
                <li>Customer landing table (unique on customer_id)</li>
                <li>Any other intermediate models you've built</li>
            </ul>
        </div>
    </div>

    <!-- Challenge 6: Late Arriving Events - Ingestion Time Tracking -->
    <div class="challenge-box">
        <h2>üéØ Challenge 6: Handle Late-Arriving Data with Ingestion Time Tracking</h2>
        <p>Implement ingestion time tracking for late-arriving data discussed in today's presentation.</p>
        
        <div class="info-box">
            <h3>üìÖ The Late Data Problem (from Session 2 Presentation)</h3>
            <p><strong>The Scenario:</strong> Payment confirmations arrive 1-3 days after the original order.</p>
            <p><strong>The Risk:</strong> Training data includes all payments, but inference data is missing recent late arrivals.</p>
            <p><strong>The Result:</strong> Training/serving skew that degrades model performance.</p>
            
            <p><strong>Solution: Ingestion Time Tracking</strong></p>
            <ul>
                <li>Track when data arrives in your system (not just when events occurred)</li>
                <li>Ensures point-in-time correctness for model training</li>
                <li>Best for mission-critical systems requiring perfect accuracy</li>
            </ul>
        </div>
        
        <h3>Requirements:</h3>
        <ol>
            <li>Add ingestion time tracking to staging tables</li>
            <li>Build point-in-time correct features</li>
            <li>Test with simulated late data</li>
            <li>Verify point-in-time accuracy</li>
        </ol>
        
        <button class="solution-button" onclick="toggleSolution('solution6')">Show Solution</button>
        <div id="solution6" class="solution">
            <h4>Step 1: Add Ingestion Time Tracking to Staging</h4>
            <p><strong>models/staging/stg_orders_with_ingestion.sql</strong></p>
            <pre>
{{ config(
    materialized='incremental',
    unique_key='order_id',
    incremental_strategy='merge'
) }}

SELECT
    order_id,
    customer_id,
    order_status,
    order_purchase_timestamp::timestamp as order_purchase_timestamp,
    DATE(order_purchase_timestamp) as order_date,
    
    -- INGESTION TIME STRATEGY: Track when we received this data
    CURRENT_TIMESTAMP as ingestion_time,
    
    -- Simulate late arrival: some orders arrive 1-3 days after order_date
    CASE 
        WHEN order_id IN (
            SELECT order_id FROM {{ source('olist_data', 'olist_orders_dataset') }}
            WHERE MOD(ABS(HASHTEXT(order_id)), 10) = 0  -- 10% of orders
        ) THEN order_purchase_timestamp + INTERVAL '2 days'
        ELSE order_purchase_timestamp + INTERVAL '1 hour'
    END as simulated_arrival_time

FROM {{ source('olist_data', 'olist_orders_dataset') }}

WHERE order_status != 'unavailable'
  AND order_purchase_timestamp IS NOT NULL
  
  {% if is_incremental() %}
    -- Only process recently arrived data
    AND order_purchase_timestamp > (
        SELECT MAX(order_purchase_timestamp) - INTERVAL '1 day'
        FROM {{ this }}
    )
  {% endif %}</pre>
            
            <h4>Step 2: Build Point-in-Time Correct Features</h4>
            <p><strong>models/intermediate/int_payment_features_ingestion.sql</strong></p>
            <pre>
{{ config(materialized='table') }}

-- Point-in-time correct features using ingestion time
WITH prediction_dates AS (
    {{ dbt_utils.date_spine(
        datepart="day",
        start_date="'2017-01-01'::date",
        end_date="'2017-03-31'::date"
    ) }}
),

point_in_time_payments AS (
    SELECT 
        pd.date_day as prediction_date,
        o.customer_id,
        COUNT(DISTINCT o.order_id) as orders_count,
        SUM(p.payment_value) as total_payments
    FROM prediction_dates pd
    CROSS JOIN {{ ref('stg_orders_with_ingestion') }} o
    INNER JOIN {{ ref('stg_order_payments') }} p
        ON o.order_id = p.order_id
    WHERE o.order_status NOT IN ('canceled', 'unavailable')
      -- CRITICAL: Only use data we "knew" at prediction time
      AND o.simulated_arrival_time <= pd.date_day + INTERVAL '1 day'
      AND DATE(o.order_purchase_timestamp) <= pd.date_day
    GROUP BY 1, 2
)

SELECT 
    prediction_date as date,
    customer_id,
    COALESCE(orders_count, 0) as orders_count,
    COALESCE(total_payments, 0) as total_payments,
    'ingestion_time_tracking' as method
FROM prediction_dates pd
LEFT JOIN point_in_time_payments pit
    ON pd.date_day = pit.prediction_date</pre>
            
            <h4>Step 3: Test the Ingestion Time Tracking</h4>
            <pre>
# Build staging table with ingestion time tracking
dbt run --select stg_orders_with_ingestion --full-refresh

# Build point-in-time correct features
dbt run --select int_payment_features_ingestion

# Verify point-in-time correctness in your database:
# SELECT 
#     date,
#     customer_id,
#     orders_count,
#     total_payments
# FROM int_payment_features_ingestion
# ORDER BY date, customer_id
# LIMIT 100;</pre>
            
            <div class="info-box">
                <h3>üìä Key Benefits of Ingestion Time Tracking:</h3>
                
                <ul>
                    <li><strong>Perfect Point-in-Time Accuracy:</strong> Features reflect exactly what data was available at prediction time</li>
                    <li><strong>Eliminates Training/Serving Skew:</strong> Training and inference use the same data availability logic</li>
                    <li><strong>Regulatory Compliance:</strong> Audit trail of when data entered the system</li>
                    <li><strong>Late Data Visibility:</strong> Can measure and monitor data arrival patterns</li>
                    <li><strong>Best for:</strong> Financial systems, high-stakes ML models, mission-critical applications</li>
                </ul>
                
                <p><strong>Important Considerations:</strong></p>
                <ul>
                    <li>Requires schema changes to add ingestion time columns</li>
                    <li>More complex to implement than simple lookback windows</li>
                    <li>May require additional storage for tracking metadata</li>
                    <li>Worth the complexity when accuracy is critical</li>
                </ul>
            </div>
        </div>
        
        <div class="optional-challenge">
            <h3>üåü Optional Challenge: Enhanced Ingestion Time Tracking</h3>
            <p>Extend the ingestion time tracking approach:</p>
            <ul>
                <li>Add data freshness monitoring: track time between order_date and ingestion_time</li>
                <li>Create alerts for unusually late data arrivals</li>
                <li>Build a dashboard showing data arrival patterns over time</li>
                <li>Add version tracking to handle data corrections and updates</li>
                <li>Implement data quality checks at ingestion time</li>
            </ul>
        </div>
    </div>

    <!-- Summary -->
    <div class="info-box">
        <h2>üéâ Session 2 Complete!</h2>
        
        <h3>What You've Built:</h3>
        <ul>
            <li>‚úÖ Simple SCD Type 2 tracking with snapshots</li>
            <li>‚úÖ Mastered dbt variable hierarchy and resolution</li>
            <li>‚úÖ Created reusable macros (RFM analysis!)</li>
            <li>‚úÖ Built and tested incremental models properly</li>
            <li>‚úÖ Implemented build control for testing</li>
            <li>‚úÖ Handled late-arriving data with ingestion time tracking</li>
        </ul>
        
        <h3>Key Concepts Mastered:</h3>
        <ul>
            <li>Slowly changing dimensions with snapshots</li>
            <li>Variable hierarchy (project ‚Üí command line)</li>
            <li>Macro creation and reusability</li>
            <li>Incremental strategies and testing</li>
            <li>Late-arriving data patterns with ingestion time tracking</li>
            <li>Build timestamp control for testing</li>
        </ul>
        
        <h3>Production Readiness Achieved:</h3>
        <ul>
            <li>Models handle new data incrementally (10-100x faster)</li>
            <li>Historical changes tracked properly</li>
            <li>Late data won't cause training/serving skew</li>
            <li>Configuration is flexible and environment-aware</li>
            <li>Business logic reusable through macros</li>
        </ul>
        
        <h3>Alignment with Session 2 Presentation:</h3>
        <ul>
            <li>‚úÖ Addressed late-arriving data as the primary ML killer</li>
            <li>‚úÖ Implemented ingestion time tracking for late-arriving data</li>
            <li>‚úÖ Demonstrated incremental strategies from the presentation</li>
            <li>‚úÖ Added proper testing workflows for incremental models</li>
            <li>‚úÖ Built point-in-time correctness into feature pipelines</li>
        </ul>
        
        <h3>Next Session Preview:</h3>
        <p>In Session 3, we'll add:</p>
        <ul>
            <li>Comprehensive testing (unit tests, schema tests, custom tests)</li>
            <li>Data freshness monitoring and alerts</li>
            <li>Pre/post hooks for automation</li>
            <li>Seeds for reference data management</li>
            <li>Production deployment with Feast integration</li>
            <li>Your 30-day implementation roadmap</li>
        </ul>
        
        <div class="warning-box">
            <strong>Before Next Session:</strong> Ensure all incremental models run successfully and your snapshots are capturing changes. You'll add comprehensive testing and monitoring on top of them!
        </div>
    </div>
</body>
</html>