<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Session 2: Advanced Patterns - Hands-on Exercises</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #e74c3c;
            padding-bottom: 10px;
        }
        
        h2 {
            color: #34495e;
            margin-top: 30px;
            border-left: 4px solid #e74c3c;
            padding-left: 10px;
        }
        
        h3 {
            color: #7f8c8d;
            margin-top: 20px;
        }
        
        .info-box {
            background-color: #e8f4fd;
            border-left: 4px solid #3498db;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }
        
        .warning-box {
            background-color: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }
        
        .challenge-box {
            background-color: #fff;
            border: 2px solid #e74c3c;
            padding: 20px;
            margin: 30px 0;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        .optional-challenge {
            background-color: #ffeaa7;
            border: 2px dashed #fdcb6e;
            padding: 15px;
            margin: 15px 0;
            border-radius: 8px;
        }
        
        code {
            background-color: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
        }
        
        pre {
            background-color: #2c3e50;
            color: #ecf0f1;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            margin: 15px 0;
        }
        
        .solution {
            display: none;
            background-color: #e8f8f5;
            border: 2px solid #27ae60;
            padding: 15px;
            margin-top: 10px;
            border-radius: 5px;
        }
        
        .solution-button {
            background-color: #e74c3c;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 5px;
            cursor: pointer;
            margin: 10px 0;
            font-size: 14px;
        }
        
        .solution-button:hover {
            background-color: #c0392b;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
            background-color: white;
        }
        
        th, td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        
        th {
            background-color: #e74c3c;
            color: white;
        }
        
        tr:nth-child(even) {
            background-color: #f2f2f2;
        }
        
        .highlight {
            background-color: #ffeb3b;
            padding: 2px 4px;
            border-radius: 3px;
        }
        
        .recap-box {
            background-color: #f0f3f4;
            border: 2px solid #34495e;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }
    </style>
    <script>
        function toggleSolution(id) {
            var solution = document.getElementById(id);
            var button = event.target;
            if (solution.style.display === 'none' || solution.style.display === '') {
                solution.style.display = 'block';
                button.textContent = 'Hide Solution';
            } else {
                solution.style.display = 'none';
                button.textContent = 'Show Solution';
            }
        }
    </script>
</head>
<body>
    <h1>üöÄ Session 2: Advanced Patterns - Hands-on Exercises</h1>
    
    <div class="info-box">
        <h3>‚è±Ô∏è Duration: 1-1.5 hours</h3>
        <p><strong>Prerequisites:</strong> Completed Session 1 models running successfully</p>
        <p><strong>Goal:</strong> Add advanced features: snapshots, incremental models, macros, and late-arriving data handling</p>
    </div>

    <div class="recap-box">
        <h2>üìã Quick Recap from Session 1</h2>
        <p><strong>What We Built:</strong></p>
        <ul>
            <li>‚úÖ Staging tables (<code>stg_orders</code>, <code>stg_customers</code>, etc.)</li>
            <li>‚úÖ Customer landing table (<code>int_customer_landing</code>)</li>
            <li>‚úÖ Daily feature table (<code>int_customer_daily_features</code>)</li>
            <li>‚úÖ Used sources, refs, and Jinja templating</li>
        </ul>
        <p><strong>Today's Focus:</strong> Make it production-ready with incremental processing and handle changing dimensions!</p>
    </div>

    <div class="info-box">
        <h3>üí° Need to Catch Up or Reset?</h3>
        <p>If you're joining this session or want to start fresh with all Session 1 models in place, you can use the reset script:</p>
        <pre>./reset_to_session.sh 1</pre>
        <p>This will reset your project to the end of Session 1 with all foundation models ready to go.</p>
        <p><strong>‚ö†Ô∏è Use Wisely:</strong> The reset script will overwrite any custom work or bonus challenges you've completed. Make sure to save any extra code you want to keep before running it!</p>
    </div>

    <h2>üîß Variables in dbt</h2>
    
    <div class="info-box">
        <h3>Understanding dbt Variables Hierarchy</h3>
        
        <p><strong>What are dbt Variables?</strong></p>
        <p>Variables allow you to pass values from outside into your models, making them configurable.</p>
        
        <p><strong>Variable Hierarchy (Bottom Wins):</strong></p>
        <ol>
            <li><strong>Project Level:</strong> <code>dbt_project.yml</code> vars section</li>
            <li><strong>Environment/Profile Level:</strong> <code>profiles.yml</code> vars</li>
            <li><strong>Command Line:</strong> <code>--vars</code> flag (highest priority)</li>
        </ol>
        
        <p><strong>Examples:</strong></p>
        <pre>
# 1. Project level (dbt_project.yml)
vars:
  lookback_days: [3, 7, 14]
  churn_threshold: 90

# 2. Command line (highest priority)
dbt run --vars '{lookback_days: [7, 14, 30], churn_threshold: 120}'

# 3. In model (with default fallback)
{% set lookback_days = var('lookback_days', [3, 7, 14]) %}
{% set threshold = var('churn_threshold', 90) %}</pre>
        
        <p><strong>Priority Order:</strong> Command Line > Profile > Project > Model Default</p>
    </div>

    <!-- Challenge 1: Variables - ENHANCED -->
    <div class="challenge-box">
        <h2>üéØ Challenge 1: Master Variable Hierarchy</h2>
        <p>Learn how dbt resolves variables from different sources and test the hierarchy in action.</p>
        
        <h3>Requirements:</h3>
        <ol>
            <li>Set up variables at project level</li>
            <li>Override with command line variables</li>
            <li>Update the daily features model to use variables</li>
            <li>Test variable resolution priority</li>
        </ol>
        
        <button class="solution-button" onclick="toggleSolution('solution1')">Show Solution</button>
        <div id="solution1" class="solution">
            <h4>Step 1: Add to dbt_project.yml (Project Level)</h4>
            <pre>
# Add this to your dbt_project.yml
vars:
  lookback_days: [3, 7, 14]
  max_date: '2018-10-31'
  churn_threshold_days: 90
  min_order_value: 10</pre>
            
            <h4>Step 2: Update models/intermediate/int_customer_daily_features.sql</h4>
            <pre>
{{ config(materialized='table') }}

-- Use variables with hierarchy demonstration
{% set lookback_days = var('lookback_days') %}
{% set max_date = var('max_date') %}
{% set min_order_value = var('min_order_value', 0) %}

-- Log variable values to see what's being used
{{ log("Using lookback_days: " ~ lookback_days, info=True) }}
{{ log("Max feature date: " ~ max_date, info=True) }}
{{ log("Min order value: " ~ min_order_value, info=True) }}

WITH customer_dates AS (
    SELECT 
        c.customer_id,
        c.landing_date,
        d.date_day as date
    FROM {{ ref('int_customer_landing') }} c
    CROSS JOIN (
        {{ dbt_utils.date_spine(
            datepart="day",
            start_date="'2016-01-01'::date",
            end_date="'2018-12-31'::date"
        ) }}
    ) d
    WHERE d.date_day >= c.landing_date
      AND d.date_day <= '{{ max_date }}'::date
),

daily_payments AS (
    SELECT
        o.customer_id,
        DATE(o.order_purchase_timestamp) as order_date,
        SUM(p.payment_value) as daily_payment_value,
        COUNT(DISTINCT o.order_id) as daily_order_count
    FROM {{ ref('stg_orders') }} o
    INNER JOIN {{ ref('stg_order_payments') }} p
        ON o.order_id = p.order_id
    WHERE o.order_status NOT IN ('canceled', 'unavailable')
      AND p.payment_value >= {{ min_order_value }}  -- Using variable
    GROUP BY 1, 2
),

features AS (
    SELECT
        cd.customer_id,
        cd.date,
        cd.landing_date,
        
        -- Cumulative payment value
        SUM(COALESCE(dp.daily_payment_value, 0)) OVER (
            PARTITION BY cd.customer_id 
            ORDER BY cd.date 
            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
        ) as total_payment_value,
        
        -- Dynamic rolling window features
        {% for days in lookback_days %}
        SUM(COALESCE(dp.daily_payment_value, 0)) OVER (
            PARTITION BY cd.customer_id 
            ORDER BY cd.date 
            ROWS BETWEEN {{ days - 1 }} PRECEDING AND CURRENT ROW
        ) as payment_{{ days }}d,
        
        SUM(COALESCE(dp.daily_order_count, 0)) OVER (
            PARTITION BY cd.customer_id 
            ORDER BY cd.date 
            ROWS BETWEEN {{ days - 1 }} PRECEDING AND CURRENT ROW
        ) as orders_{{ days }}d,
        {% endfor %}
        
        -- Days metrics
        cd.date - cd.landing_date as days_since_landing
        
    FROM customer_dates cd
    LEFT JOIN daily_payments dp
        ON cd.customer_id = dp.customer_id
        AND cd.date = dp.order_date
)

SELECT * FROM features</pre>
            
            <h4>Step 3: Test Variable Hierarchy</h4>
            <pre>
# Test 1: Use project-level variables (from dbt_project.yml)
dbt run --select int_customer_daily_features
# Check logs - should show [3, 7, 14] for lookback_days

# Test 2: Override with command line (highest priority)
dbt run --select int_customer_daily_features --vars '{lookback_days: [1, 5, 10]}'
# Check logs - should show [1, 5, 10] for lookback_days

# Test 3: Partial override
dbt run --select int_customer_daily_features --vars '{max_date: "2018-06-30"}'
# lookback_days should come from project, max_date from command line

# Test 4: See compiled SQL differences
dbt compile --select int_customer_daily_features --vars '{lookback_days: [30]}'
# Check target/compiled/.../int_customer_daily_features.sql</pre>
            
            <p><strong>Variable Resolution Test:</strong></p>
            <p>Watch the dbt logs during runs to see which variable values are actually being used!</p>
        </div>
        
        <div class="optional-challenge">
            <h3>üåü Optional Challenge: Environment-Specific Variables</h3>
            <p>Set up different variables for dev vs prod environments:</p>
            <ul>
                <li>Dev: Smaller lookback windows, limited date ranges</li>
                <li>Prod: Full date ranges, longer lookback windows</li>
                <li>Use <code>target.name</code> to conditionally set variables</li>
            </ul>
            
            <p><strong>Example:</strong></p>
            <pre>
-- In your model
{% if target.name == 'dev' %}
    {% set lookback_days = [3, 7] %}
{% else %}
    {% set lookback_days = [3, 7, 14, 30] %}
{% endif %}

-- Run with: dbt run --target dev  OR  dbt run --target prod</pre>
        </div>
    </div>

    <h2>üì∏ Slowly Changing Dimensions (SCD) Type 2</h2>

    <div class="info-box">
        <h3>Understanding SCD Type 2 with dbt Snapshots</h3>

        <p><strong>What are Slowly Changing Dimensions?</strong></p>
        <p>Business attributes that change over time (seller tiers, customer segments, product categories). We need to track history for analysis and point-in-time correctness.</p>

        <p><strong>dbt Snapshots Solution:</strong></p>
        <ul>
            <li>Automatically tracks changes in source tables</li>
            <li>Creates SCD Type 2 tables with validity periods</li>
            <li>Adds <code>dbt_valid_from</code> and <code>dbt_valid_to</code> columns</li>
            <li>Runs on a schedule to capture state changes</li>
        </ul>

        <p><strong>How it works:</strong></p>
        <pre>
-- First run: Takes snapshot of current state
-- Subsequent runs: Detects changes and creates new rows with validity periods
-- Result: Complete history of all state changes

seller_id | seller_tier | total_revenue | dbt_valid_from | dbt_valid_to
S001      | bronze      | 500           | 2017-01-01     | 2017-06-30
S001      | silver      | 5000          | 2017-06-30     | 2018-01-15
S001      | gold        | 12000         | 2018-01-15     | NULL</pre>

        <p><strong>Real-World Use Cases:</strong></p>
        <ul>
            <li><strong>Seller Performance:</strong> Track seller tier changes (bronze ‚Üí silver ‚Üí gold)</li>
            <li><strong>Customer Segments:</strong> Track customer lifecycle (prospect ‚Üí active ‚Üí VIP)</li>
            <li><strong>Product Pricing:</strong> Track price tier changes over time</li>
            <li><strong>Inventory Status:</strong> Track stock level categories (in-stock ‚Üí low ‚Üí out-of-stock)</li>
        </ul>
    </div>

    <!-- Challenge 2: SCD Type 2 - NEW SELLER TIER VERSION -->
    <div class="challenge-box">
        <h2>üéØ Challenge 2: Track Seller Performance with Snapshots</h2>
        <p>Build a complete SCD Type 2 solution to track how sellers grow from bronze ‚Üí silver ‚Üí gold ‚Üí platinum tier over time.</p>

        <div class="info-box">
            <h3>üìä Business Context</h3>
            <p>In this e-commerce platform, sellers start small and grow over time. We want to track their progression through performance tiers to:</p>
            <ul>
                <li>Analyze which sellers are improving vs declining</li>
                <li>Understand how long sellers stay in each tier</li>
                <li>Calculate tier-specific metrics (avg time to gold, churn rate by tier)</li>
                <li>Enable point-in-time reporting ("Who were our gold sellers in Q1 2017?")</li>
            </ul>
        </div>

        <h3>üéØ Your Mission:</h3>
        <p>You'll build 3 models to track seller tier changes. Here are the detailed requirements for each:</p>

        <h4>Part 1: Staging Model - <code>stg_order_items_snapshot.sql</code></h4>
        <p><strong>Purpose:</strong> Clean order items data by joining with orders table and filtering out invalid data.</p>
        <p><strong>Data Cleaning Requirements:</strong></p>
        <ul>
            <li>Join <code>olist_order_items</code> with <code>olist_orders</code> to get order timestamps</li>
            <li>Filter out NULL <code>order_id</code> and <code>seller_id</code></li>
            <li>Filter out orders with <code>price &lt;= 0</code> (invalid/test data)</li>
            <li>Filter out canceled/unavailable orders: <code>order_status NOT IN ('canceled', 'unavailable')</code></li>
            <li>Filter out orders with NULL <code>order_purchase_timestamp</code></li>
            <li>Convert <code>price</code> and <code>freight_value</code> to DECIMAL(10,2)</li>
            <li>Include: order_id, order_item_id, product_id, seller_id, price, freight_value, order_purchase_timestamp, order_date, order_status</li>
        </ul>

        <h4>Part 2: Intermediate Model - <code>int_seller_performance.sql</code></h4>
        <p><strong>Purpose:</strong> Calculate seller metrics and assign tier based on order volume.</p>

        <div class="warning-box">
            <h4>üîë IMPORTANT: Using Variables for Time-Based Testing</h4>
            <p>To test how sellers progress through tiers over time, you need to use a variable called <code>snapshot_date</code> that controls "how much data to include."</p>
            <p><strong>Why?</strong> In the real world, you'd run snapshots daily. For testing, we simulate this by filtering data up to different dates:</p>
            <ul>
                <li>Run 1: Include orders up to 2017-01-01 ‚Üí See mostly bronze sellers (new sellers)</li>
                <li>Run 2: Include orders up to 2017-06-30 ‚Üí Some bronze ‚Üí silver upgrades!</li>
                <li>Run 3: Include orders up to 2018-01-31 ‚Üí Gold tier sellers appear!</li>
                <li>Run 4: Include orders up to 2018-10-17 ‚Üí Platinum tier unlocked!</li>
            </ul>
            <p><strong>How to implement:</strong> At the top of your model, use <code>{% set snapshot_date = var('snapshot_date', '2018-10-17') %}</code>, then filter with <code>WHERE order_date &lt;= '{{ snapshot_date }}'::date</code></p>
        </div>

        <p><strong>Tier Classification Rules (based on order count):</strong></p>
        <ul>
            <li><strong>Platinum:</strong> 500+ orders (top 1% performers)</li>
            <li><strong>Gold:</strong> 100-499 orders (high performers)</li>
            <li><strong>Silver:</strong> 20-99 orders (growing sellers)</li>
            <li><strong>Bronze:</strong> 1-19 orders (new/small sellers)</li>
        </ul>

        <p><strong>Metrics to Calculate per Seller:</strong></p>
        <ul>
            <li><code>seller_id</code> - Primary key</li>
            <li><code>seller_tier</code> - Classification based on order count (use CASE statement)</li>
            <li><code>total_orders</code> - COUNT(DISTINCT order_id)</li>
            <li><code>unique_products</code> - COUNT(DISTINCT product_id)</li>
            <li><code>total_revenue</code> - SUM(price)</li>
            <li><code>avg_order_value</code> - AVG(price)</li>
            <li><code>first_sale_date</code> - MIN(order_date)</li>
            <li><code>last_sale_date</code> - MAX(order_date)</li>
            <li><code>updated_at</code> - CURRENT_TIMESTAMP (required for snapshot tracking)</li>
        </ul>

        <h4>Part 3: Snapshot - <code>snap_seller_tier.sql</code></h4>
        <p><strong>Purpose:</strong> Automatically track when sellers change tiers using dbt's snapshot functionality.</p>
        <p><strong>Snapshot Configuration:</strong></p>
        <ul>
            <li><code>target_schema</code>: 'olist_data' (keep snapshots in the regular schema)</li>
            <li><code>strategy</code>: 'timestamp' (detect changes based on timestamp column)</li>
            <li><code>unique_key</code>: 'seller_id' (one row per seller)</li>
            <li><code>updated_at</code>: 'updated_at' (column to monitor for changes)</li>
        </ul>

        <h4>Part 4: Testing Workflow</h4>
        <p>Follow these steps to see sellers progress through tiers over time:</p>
        <ol>
            <li>Build the staging model once</li>
            <li>Build performance model with <code>snapshot_date = "2017-01-01"</code>, then take snapshot</li>
            <li>Rebuild performance with <code>snapshot_date = "2017-06-30"</code>, then snapshot again ‚Üí Detect changes!</li>
            <li>Continue with 2018-01-31 and 2018-10-17 to see full progression</li>
        </ol>

        <button class="solution-button" onclick="toggleSolution('solution2')">Show Solution</button>
        <div id="solution2" class="solution">
            <h4>Part 1: models/staging/stg_order_items_snapshot.sql</h4>
            <pre>
{{ config(materialized='table') }}

SELECT
    oi.order_id,
    oi.order_item_id,
    oi.product_id,
    oi.seller_id,
    oi.price::DECIMAL(10,2) as price,
    oi.freight_value::DECIMAL(10,2) as freight_value,
    o.order_purchase_timestamp::timestamp as order_purchase_timestamp,
    DATE(o.order_purchase_timestamp) as order_date,
    o.order_status
FROM {{ source('olist_data', 'olist_order_items') }} oi
INNER JOIN {{ source('olist_data', 'olist_orders') }} o
    ON oi.order_id = o.order_id
WHERE oi.order_id IS NOT NULL
  AND oi.seller_id IS NOT NULL
  AND oi.price > 0
  AND o.order_status NOT IN ('canceled', 'unavailable')
  AND o.order_purchase_timestamp IS NOT NULL</pre>

            <h4>Part 2: models/intermediate/int_seller_performance.sql</h4>
            <pre>
{{ config(materialized='table') }}

-- Use variable to control analysis date for testing
{% set snapshot_date = var('snapshot_date', '2018-10-17') %}

WITH seller_metrics AS (
    SELECT
        seller_id,
        COUNT(DISTINCT order_id) as total_orders,
        COUNT(DISTINCT product_id) as unique_products,
        SUM(price) as total_revenue,
        AVG(price) as avg_order_value,
        MIN(order_date) as first_sale_date,
        MAX(order_date) as last_sale_date,

        -- Calculate tier based on order volume
        CASE
            WHEN COUNT(DISTINCT order_id) >= 500 THEN 'platinum'
            WHEN COUNT(DISTINCT order_id) >= 100 THEN 'gold'
            WHEN COUNT(DISTINCT order_id) >= 20 THEN 'silver'
            ELSE 'bronze'
        END as seller_tier,

        -- Track when this calculation was made
        CURRENT_TIMESTAMP as updated_at

    FROM {{ ref('stg_order_items_snapshot') }}
    WHERE order_date <= '{{ snapshot_date }}'::date
    GROUP BY seller_id
)

SELECT
    seller_id,
    seller_tier,
    total_orders,
    unique_products,
    total_revenue,
    avg_order_value,
    first_sale_date,
    last_sale_date,
    updated_at
FROM seller_metrics</pre>

            <h4>Part 3: snapshots/snap_seller_tier.sql</h4>
            <pre>
{% snapshot snap_seller_tier %}
    {{
        config(
          target_schema='olist_data',
          strategy='timestamp',
          unique_key='seller_id',
          updated_at='updated_at',
        )
    }}

    SELECT * FROM {{ ref('int_seller_performance') }}

{% endsnapshot %}</pre>

            <h4>Part 4: Testing Commands</h4>
            <pre>
# Step 1: Build staging model
dbt run --select stg_order_items_snapshot

# Step 2: Initial snapshot (Jan 2017)
dbt run --select int_seller_performance --vars '{snapshot_date: "2017-01-01"}'
dbt snapshot --select snap_seller_tier

# Step 3: Second snapshot (Jun 2017) - Detect tier changes!
dbt run --select int_seller_performance --vars '{snapshot_date: "2017-06-30"}'
dbt snapshot --select snap_seller_tier

# Step 4: Third snapshot (Jan 2018)
dbt run --select int_seller_performance --vars '{snapshot_date: "2018-01-31"}'
dbt snapshot --select snap_seller_tier

# Step 5: Final snapshot (Oct 2018)
dbt run --select int_seller_performance --vars '{snapshot_date: "2018-10-17"}'
dbt snapshot --select snap_seller_tier</pre>

            <h4>Verification Queries</h4>

            <p><strong>1. View seller tier distribution:</strong></p>
            <pre>
SELECT seller_tier, COUNT(*) as seller_count
FROM olist_data.snap_seller_tier
WHERE dbt_valid_to IS NULL  -- Current state only
GROUP BY seller_tier;</pre>

            <p><strong>2. Find sellers who changed tiers (SCD Type 2 proof!):</strong></p>
            <pre>
SELECT seller_id, COUNT(*) as tier_changes
FROM olist_data.snap_seller_tier
GROUP BY seller_id
HAVING COUNT(*) > 1
ORDER BY tier_changes DESC
LIMIT 10;</pre>

            <p><strong>3. See a specific seller's progression over time:</strong></p>
            <pre>
SELECT
    seller_tier,
    total_orders,
    dbt_valid_from,
    dbt_valid_to
FROM olist_data.snap_seller_tier
WHERE seller_id = 'YOUR_SELLER_ID_HERE'
ORDER BY dbt_valid_from;</pre>

            <p><strong>Expected Results:</strong> ~2,000 sellers with tier changes, showing bronze ‚Üí silver ‚Üí gold ‚Üí platinum progressions!</p>
        </div>

        <div class="optional-challenge">
            <h3>üåü Optional Challenge: Seller Cohort Analysis</h3>
            <p>Use the snapshot to analyze seller lifecycle:</p>
            <ul>
                <li>Calculate average time to reach each tier (e.g., "90 days to silver")</li>
                <li>Identify sellers who were downgraded (gold ‚Üí silver)</li>
                <li>Create a seller churn analysis (stopped getting orders)</li>
                <li>Build a "tier transition matrix" showing bronze‚Üísilver‚Üígold flow</li>
                <li>Calculate revenue by tier at different points in time</li>
            </ul>
        </div>
    </div>

    <div class="info-box">
        <h4>üìù What does --full-refresh do?</h4>
        <p>The <code>--full-refresh</code> flag forces dbt to completely rebuild a model from scratch, dropping and recreating the table. This is especially important for incremental models:</p>
        <ul>
            <li><strong>For Incremental Models:</strong> Rebuilds the entire table from scratch, ignoring incremental logic</li>
            <li><strong>When Model Logic Changes:</strong> Recomputes all historical data with new logic</li>
            <li><strong>When Adding Columns:</strong> Ensures new columns are calculated for all existing rows</li>
            <li><strong>For Testing:</strong> Lets you reset and rebuild with controlled data subsets</li>
        </ul>
        <p><strong>Without --full-refresh:</strong> Incremental models only process new/changed data based on the <code>is_incremental()</code> logic.</p>
        <p><strong>With --full-refresh:</strong> The model behaves like a regular table materialization, processing all data from scratch.</p>
    </div>

    <h2>üìà Incremental Models - The Production Game Changer</h2>
    
    <div class="info-box">
        <h3>Understanding Incremental Models from Session 2 Presentation</h3>
        
        <p><strong>Key Concepts from Today's Presentation:</strong></p>
        <ul>
            <li><strong>Problem:</strong> Processing 100M orders daily is expensive and slow</li>
            <li><strong>Solution:</strong> Only process new/changed data (10-100x faster)</li>
            <li><strong>Strategies:</strong> Append, Merge, Delete+Insert</li>
            <li><strong>Late Data:</strong> Use lookback windows or ingestion time tracking</li>
        </ul>
        
        <p><strong>How to Test Incremental Models Properly:</strong></p>
        <ol>
            <li><strong>Full Refresh + Limited Data:</strong> Build table with subset of data</li>
            <li><strong>Check Compiled SQL:</strong> Verify the CREATE TABLE statement</li>
            <li><strong>Full Refresh + More Data:</strong> Rebuild with more data</li>
            <li><strong>Incremental Run:</strong> Run without --full-refresh</li>
            <li><strong>Check Incremental SQL:</strong> Verify the MERGE/INSERT statement</li>
            <li><strong>Validate Data:</strong> Ensure only new data was processed</li>
        </ol>
        
        <div class="warning-box">
            <strong>Live Demo Alert!</strong> During class, we've demonstrated this step-by-step process live, showing you exactly how to inspect the compiled SQL and validate incremental behavior.
        </div>
    </div>

    <!-- Challenge 3: Incremental Models -->
    <div class="challenge-box">
        <h2>üéØ Challenge 3: Build and Test Incremental Models</h2>
        <p>Convert your feature models to incremental and learn to test them properly.</p>
        
        <h3>Requirements:</h3>
        <ol>
            <li>Create a new incremental model <code>int_customer_daily_features_inc.sql</code> (similar to <code>int_customer_daily_features.sql</code> but with incremental logic)</li>
            <li>Follow the proper testing workflow</li>
            <li>Verify incremental behavior</li>
        </ol>
        
        <div class="info-box">
            <strong>üí° Reminder:</strong> The <code>{{ this }}</code> keyword in dbt refers to the current model's table in the database. When used inside an incremental model, it lets you query the existing table to find the maximum date or other metadata to determine what new data to process.
        </div>
        
        <button class="solution-button" onclick="toggleSolution('solution3')">Show Solution</button>
        <div id="solution3" class="solution">
            <h4>models/intermediate/int_customer_daily_features_inc.sql (Incremental Version)</h4>
            <pre>
{{ config(
    materialized='incremental',
    unique_key=['customer_id', 'date'],
    incremental_strategy='merge'
) }}

-- Use variables with hierarchy demonstration
{% set lookback_days = var('lookback_days', [3, 7, 14]) %}
{% set max_date = var('max_date', '2018-10-31') %}

WITH customer_dates AS (
    SELECT 
        c.customer_id,
        c.landing_date,
        d.date_day as date
    FROM {{ ref('int_customer_landing') }} c
    CROSS JOIN (
        {{ dbt_utils.date_spine(
            datepart="day",
            start_date="'2016-01-01'::date",
            end_date="'2018-12-31'::date"
        ) }}
    ) d
    WHERE d.date_day >= c.landing_date
      AND d.date_day <= '{{ max_date }}'::date
      
    {% if is_incremental() %}
      -- Only process recent feature dates for incremental runs
      AND d.date_day >= (
        SELECT MAX(date) - INTERVAL '7 days'
        FROM {{ this }}
      )
    {% endif %}
),

daily_payments AS (
    SELECT
        o.customer_id,
        DATE(o.order_purchase_timestamp) as order_date,
        SUM(p.payment_value) as daily_payment_value,
        COUNT(DISTINCT o.order_id) as daily_order_count
    FROM {{ ref('stg_orders') }} o
    INNER JOIN {{ ref('stg_order_payments') }} p
        ON o.order_id = p.order_id
    WHERE o.order_status NOT IN ('canceled', 'unavailable')
    
    {% if is_incremental() %}
      -- Look back for late-arriving data
      AND DATE(o.order_purchase_timestamp) >= (
        SELECT MAX(date) - INTERVAL '14 days'
        FROM {{ this }}
      )
    {% endif %}
    
    GROUP BY 1, 2
),

features AS (
    SELECT
        cd.customer_id,
        cd.date,
        cd.landing_date,
        
        -- Cumulative payment value
        SUM(COALESCE(dp.daily_payment_value, 0)) OVER (
            PARTITION BY cd.customer_id 
            ORDER BY cd.date 
            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
        ) as total_payment_value,
        
        -- Dynamic rolling window features
        {% for days in lookback_days %}
        SUM(COALESCE(dp.daily_payment_value, 0)) OVER (
            PARTITION BY cd.customer_id 
            ORDER BY cd.date 
            ROWS BETWEEN {{ days - 1 }} PRECEDING AND CURRENT ROW
        ) as payment_{{ days }}d,
        
        SUM(COALESCE(dp.daily_order_count, 0)) OVER (
            PARTITION BY cd.customer_id 
            ORDER BY cd.date 
            ROWS BETWEEN {{ days - 1 }} PRECEDING AND CURRENT ROW
        ) as orders_{{ days }}d,
        {% endfor %}
        
        -- Days metrics
        cd.date - cd.landing_date as days_since_landing
        
    FROM customer_dates cd
    LEFT JOIN daily_payments dp
        ON cd.customer_id = dp.customer_id
        AND cd.date = dp.order_date
)

SELECT * FROM features</pre>
            
            <h4>Proper Testing Workflow (Step by Step):</h4>
            <pre>
# STEP 1: Full refresh with limited data (January 2017)
dbt run --select int_customer_daily_features_inc --full-refresh \
  --vars '{max_date: "2017-01-31"}'

# Check what was built:
# SELECT MIN(date), MAX(date), COUNT(*)
# FROM int_customer_daily_features_inc;

# STEP 2: Check the compiled SQL (CREATE TABLE)
cat target/run/dbt_workshop/models/intermediate/int_customer_daily_features_inc.sql
# Should see CREATE TABLE statement, no is_incremental() logic

# STEP 3: Full refresh with more data (February 2017)
dbt run --select int_customer_daily_features_inc --full-refresh \
  --vars '{max_date: "2017-02-28"}'

# Verify more data was added

# STEP 4: Now test incremental behavior (March 2017)
dbt run --select int_customer_daily_features_inc \
  --vars '{max_date: "2017-03-31"}'

# STEP 5: Check the incremental SQL (MERGE statement)
cat target/run/dbt_workshop/models/intermediate/int_customer_daily_features_inc.sql
# Should see MERGE statement with incremental logic

# STEP 6: Verify only new data was processed
# Count should increase, but only with March data</pre>
            
            <div class="info-box">
                <h3>üîç What to Look for in Compiled SQL:</h3>
                <p><strong>Full Refresh:</strong></p>
                <ul>
                    <li><code>CREATE TABLE</code> statement</li>
                    <li>No <code>is_incremental()</code> logic in WHERE clauses</li>
                    <li>Processes all data within date limits</li>
                </ul>
                
                <p><strong>Incremental Run:</strong></p>
                <ul>
                    <li><code>MERGE</code> or <code>INSERT</code> statement</li>
                    <li>Incremental logic in WHERE clauses</li>
                    <li>Only recent data processed</li>
                </ul>
            </div>
            
            <p><strong>Validate the Magic Happened:</strong></p>
            <pre>
-- After incremental run, check the lookback logic worked:
 SELECT 
    date, 
    COUNT(*)
 FROM int_customer_daily_features_inc
 WHERE date >= '2017-02-15'  -- Should see lookback data
 GROUP BY date
 ORDER BY date;
</pre>
        </div>
        
        <div class="optional-challenge">
            <h3>üåü Optional Challenge: Convert All Models to Incremental</h3>
            <p>Apply incremental strategies to:</p>
            <ul>
                <li>All staging models (with appropriate strategies)</li>
                <li>Customer landing table (unique on customer_id)</li>
                <li>Any other intermediate models you've built</li>
            </ul>
        </div>
    </div>

    <!-- Challenge 4: Late Arriving Events - Ingestion Time Tracking -->
    <div class="challenge-box">
        <h2>üéØ Challenge 4: Handle Late-Arriving Data with Ingestion Time Tracking</h2>
        <p>Implement ingestion time tracking for late-arriving data discussed in today's presentation.</p>
        
        <div class="info-box">
            <h3>üìÖ The Late Data Problem (from Session 2 Presentation)</h3>
            <p><strong>The Scenario:</strong> Payment confirmations arrive 1-3 days after the original order.</p>
            <p><strong>The Risk:</strong> Training data includes all payments, but inference data is missing recent late arrivals.</p>
            <p><strong>The Result:</strong> Training/serving skew that degrades model performance.</p>
            
            <p><strong>Solution: Ingestion Time Tracking</strong></p>
            <ul>
                <li>Track when data arrives in your system (not just when events occurred)</li>
                <li>Ensures point-in-time correctness for model training</li>
                <li>Best for mission-critical systems requiring perfect accuracy</li>
            </ul>
        </div>
        
        <h3>Requirements:</h3>
        <ol>
            <li>Create the provided staging model with ingestion time tracking (see below)</li>
            <li>Build point-in-time correct features using the staging model</li>
            <li>Test with simulated late data</li>
            <li>Verify point-in-time accuracy</li>
        </ol>
        
        <div class="info-box">
            <h4>üì¶ Provided Staging Model</h4>
            <p>Create this file and run it first - we've provided it so you can focus on the feature engineering:</p>
            <p><strong>models/staging/stg_orders_with_ingestion.sql</strong></p>
            <pre>
{{ config(
    materialized='incremental',
    unique_key='order_id',
    incremental_strategy='merge'
) }}

SELECT
    order_id,
    customer_id,
    order_status,
    order_purchase_timestamp::timestamp as order_purchase_timestamp,
    DATE(order_purchase_timestamp::timestamp) as order_date,
    
    -- INGESTION TIME STRATEGY: Track when we received this data
    CURRENT_TIMESTAMP as ingestion_time,
    
    -- Simulate late arrival: some orders arrive 1-3 days after order_date
    CASE 
        WHEN order_id IN (
            SELECT order_id FROM {{ source('olist_data', 'olist_orders') }}
            WHERE MOD(ABS(HASHTEXT(order_id)), 10) = 0  -- 10% of orders
        ) THEN order_purchase_timestamp::timestamp + INTERVAL '2 days'
        ELSE order_purchase_timestamp::timestamp + INTERVAL '1 hour'
    END as simulated_arrival_time

FROM {{ source('olist_data', 'olist_orders') }}

WHERE order_status != 'unavailable'
  AND order_purchase_timestamp IS NOT NULL
  
  {% if is_incremental() %}
    -- Only process recently arrived data
    AND order_purchase_timestamp::timestamp > (
        SELECT MAX(order_purchase_timestamp) - INTERVAL '1 day'
        FROM {{ this }}
    )
  {% endif %}</pre>
            
            <p><strong>Run this model first:</strong></p>
            <pre>dbt run --select stg_orders_with_ingestion --full-refresh</pre>
        </div>
        
        <button class="solution-button" onclick="toggleSolution('solution4')">Show Solution</button>
        <div id="solution4" class="solution">
            <h4>Build Point-in-Time Correct Features</h4>
            <p><strong>models/intermediate/int_payment_features_ingestion.sql</strong></p>
            <pre>
{{ config(materialized='table') }}

-- Point-in-time correct features using ingestion time
WITH prediction_dates AS (
    {{ dbt_utils.date_spine(
        datepart="day",
        start_date="'2017-01-01'::date",
        end_date="'2017-03-31'::date"
    ) }}
),

point_in_time_payments AS (
    SELECT 
        pd.date_day as prediction_date,
        o.customer_id,
        COUNT(DISTINCT o.order_id) as orders_count,
        SUM(p.payment_value) as total_payments
    FROM prediction_dates pd
    CROSS JOIN {{ ref('stg_orders_with_ingestion') }} o
    INNER JOIN {{ ref('stg_order_payments') }} p
        ON o.order_id = p.order_id
    WHERE o.order_status NOT IN ('canceled', 'unavailable')
      -- CRITICAL: Only use data we "knew" at prediction time
      AND o.simulated_arrival_time <= pd.date_day + INTERVAL '1 day'
      AND DATE(o.order_purchase_timestamp) <= pd.date_day
    GROUP BY 1, 2
)

SELECT 
    prediction_date as date,
    customer_id,
    COALESCE(orders_count, 0) as orders_count,
    COALESCE(total_payments, 0) as total_payments,
    'ingestion_time_tracking' as method
FROM prediction_dates pd
LEFT JOIN point_in_time_payments pit
    ON pd.date_day = pit.prediction_date</pre>
            
            <h4>Test the Ingestion Time Tracking</h4>
            <pre>
# Build point-in-time correct features
dbt run --select int_payment_features_ingestion

-- Verify point-in-time correctness in your database:
 SELECT 
    date,
    customer_id,
    orders_count,
    total_payments
 FROM int_payment_features_ingestion
 ORDER BY date, customer_id
 LIMIT 100;</pre>
            
            <div class="info-box">
                <h3>üìä Key Benefits of Ingestion Time Tracking:</h3>
                
                <ul>
                    <li><strong>Perfect Point-in-Time Accuracy:</strong> Features reflect exactly what data was available at prediction time</li>
                    <li><strong>Eliminates Training/Serving Skew:</strong> Training and inference use the same data availability logic</li>
                    <li><strong>Regulatory Compliance:</strong> Audit trail of when data entered the system</li>
                    <li><strong>Late Data Visibility:</strong> Can measure and monitor data arrival patterns</li>
                    <li><strong>Best for:</strong> Financial systems, high-stakes ML models, mission-critical applications</li>
                </ul>
                
                <p><strong>Important Considerations:</strong></p>
                <ul>
                    <li>Requires schema changes to add ingestion time columns</li>
                    <li>More complex to implement than simple lookback windows</li>
                    <li>May require additional storage for tracking metadata</li>
                    <li>Worth the complexity when accuracy is critical</li>
                </ul>
            </div>
        </div>
        
        <div class="optional-challenge">
            <h3>üåü Optional Challenge: Enhanced Ingestion Time Tracking</h3>
            <p>Extend the ingestion time tracking approach:</p>
            <ul>
                <li>Add data freshness monitoring: track time between order_date and ingestion_time</li>
                <li>Create alerts for unusually late data arrivals</li>
                <li>Build a dashboard showing data arrival patterns over time</li>
                <li>Add version tracking to handle data corrections and updates</li>
                <li>Implement data quality checks at ingestion time</li>
            </ul>
        </div>
    </div>

    <!-- Summary -->
    <div class="info-box">
        <h2>üéâ Session 2 Complete!</h2>
        
        <h3>What You've Built:</h3>
        <ul>
            <li>‚úÖ Simple SCD Type 2 tracking with snapshots</li>
            <li>‚úÖ Mastered dbt variable hierarchy and resolution</li>
            <li>‚úÖ Created reusable macros (RFM analysis!)</li>
            <li>‚úÖ Built and tested incremental models properly</li>
            <li>‚úÖ Handled late-arriving data with ingestion time tracking</li>
        </ul>
        
        <h3>Key Concepts Mastered:</h3>
        <ul>
            <li>Slowly changing dimensions with snapshots</li>
            <li>Variable hierarchy (project ‚Üí command line)</li>
            <li>Macro creation and reusability</li>
            <li>Incremental strategies and testing</li>
            <li>Late-arriving data patterns with ingestion time tracking</li>
        </ul>
        
        <h3>Production Readiness Achieved:</h3>
        <ul>
            <li>Models handle new data incrementally (10-100x faster)</li>
            <li>Historical changes tracked properly</li>
            <li>Late data won't cause training/serving skew</li>
            <li>Configuration is flexible and environment-aware</li>
            <li>Business logic reusable through macros</li>
        </ul>
        
        <h3>Alignment with Session 2 Presentation:</h3>
        <ul>
            <li>‚úÖ Addressed late-arriving data as the primary ML killer</li>
            <li>‚úÖ Implemented ingestion time tracking for late-arriving data</li>
            <li>‚úÖ Demonstrated incremental strategies from the presentation</li>
            <li>‚úÖ Added proper testing workflows for incremental models</li>
            <li>‚úÖ Built point-in-time correctness into feature pipelines</li>
        </ul>
        
        <h3>Next Session Preview:</h3>
        <p>In Session 3, we'll add:</p>
        <ul>
            <li>Comprehensive testing (unit tests, schema tests, custom tests)</li>
            <li>Data freshness monitoring and alerts</li>
            <li>Pre/post hooks for automation</li>
            <li>Seeds for reference data management</li>
            <li>Production deployment with Feast integration</li>
            <li>Your 30-day implementation roadmap</li>
        </ul>
        
        <div class="warning-box">
            <strong>Before Next Session:</strong> Ensure all incremental models run successfully and your snapshots are capturing changes. You'll add comprehensive testing and monitoring on top of them!
        </div>
    </div>
</body>
</html>